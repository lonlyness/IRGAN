{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "class Opt(object):\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 200\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.latent_dim = 100\n",
    "        self.img_size = 28\n",
    "        self.channels = 1\n",
    "        self.sample_interval = 400\n",
    "        \n",
    "opt = Opt()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--n_epochs', type=int, default=200, help='number of epochs of training')\n",
    "# parser.add_argument('--batch_size', type=int, default=64, help='size of the batches')\n",
    "# parser.add_argument('--lr', type=float, default=0.0002, help='adam: learning rate')\n",
    "# parser.add_argument('--b1', type=float, default=0.5, help='adam: decay of first order momentum of gradient')\n",
    "# parser.add_argument('--b2', type=float, default=0.999, help='adam: decay of first order momentum of gradient')\n",
    "# parser.add_argument('--n_cpu', type=int, default=8, help='number of cpu threads to use during batch generation')\n",
    "# parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
    "# parser.add_argument('--img_size', type=int, default=28, help='size of each image dimension')\n",
    "# parser.add_argument('--channels', type=int, default=1, help='number of image channels')\n",
    "# parser.add_argument('--sample_interval', type=int, default=400, help='interval betwen image samples')\n",
    "# opt = parser.parse_args()\n",
    "# print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Configure data loader\n",
    "os.makedirs('../../data/mnist', exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=opt.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.668664] [G loss: 0.711462]\n",
      "[Epoch 0/200] [Batch 1/938] [D loss: 0.584432] [G loss: 0.711248]\n",
      "[Epoch 0/200] [Batch 2/938] [D loss: 0.514955] [G loss: 0.711100]\n",
      "[Epoch 0/200] [Batch 3/938] [D loss: 0.459773] [G loss: 0.710947]\n",
      "[Epoch 0/200] [Batch 4/938] [D loss: 0.418679] [G loss: 0.710684]\n",
      "[Epoch 0/200] [Batch 5/938] [D loss: 0.391517] [G loss: 0.710402]\n",
      "[Epoch 0/200] [Batch 6/938] [D loss: 0.374058] [G loss: 0.710018]\n",
      "[Epoch 0/200] [Batch 7/938] [D loss: 0.364534] [G loss: 0.709588]\n",
      "[Epoch 0/200] [Batch 8/938] [D loss: 0.357003] [G loss: 0.708708]\n",
      "[Epoch 0/200] [Batch 9/938] [D loss: 0.353800] [G loss: 0.707378]\n",
      "[Epoch 0/200] [Batch 10/938] [D loss: 0.351912] [G loss: 0.705806]\n",
      "[Epoch 0/200] [Batch 11/938] [D loss: 0.350105] [G loss: 0.703808]\n",
      "[Epoch 0/200] [Batch 12/938] [D loss: 0.348722] [G loss: 0.701749]\n",
      "[Epoch 0/200] [Batch 13/938] [D loss: 0.349930] [G loss: 0.698055]\n",
      "[Epoch 0/200] [Batch 14/938] [D loss: 0.352046] [G loss: 0.695377]\n",
      "[Epoch 0/200] [Batch 15/938] [D loss: 0.353073] [G loss: 0.690732]\n",
      "[Epoch 0/200] [Batch 16/938] [D loss: 0.354616] [G loss: 0.687014]\n",
      "[Epoch 0/200] [Batch 17/938] [D loss: 0.357752] [G loss: 0.680589]\n",
      "[Epoch 0/200] [Batch 18/938] [D loss: 0.363185] [G loss: 0.671741]\n",
      "[Epoch 0/200] [Batch 19/938] [D loss: 0.363564] [G loss: 0.670436]\n",
      "[Epoch 0/200] [Batch 20/938] [D loss: 0.365574] [G loss: 0.667968]\n",
      "[Epoch 0/200] [Batch 21/938] [D loss: 0.367586] [G loss: 0.665757]\n",
      "[Epoch 0/200] [Batch 22/938] [D loss: 0.367111] [G loss: 0.671188]\n",
      "[Epoch 0/200] [Batch 23/938] [D loss: 0.368105] [G loss: 0.672951]\n",
      "[Epoch 0/200] [Batch 24/938] [D loss: 0.367128] [G loss: 0.682021]\n",
      "[Epoch 0/200] [Batch 25/938] [D loss: 0.364389] [G loss: 0.692400]\n",
      "[Epoch 0/200] [Batch 26/938] [D loss: 0.361597] [G loss: 0.699961]\n",
      "[Epoch 0/200] [Batch 27/938] [D loss: 0.360386] [G loss: 0.705794]\n",
      "[Epoch 0/200] [Batch 28/938] [D loss: 0.357911] [G loss: 0.710467]\n",
      "[Epoch 0/200] [Batch 29/938] [D loss: 0.359724] [G loss: 0.714917]\n",
      "[Epoch 0/200] [Batch 30/938] [D loss: 0.358984] [G loss: 0.720327]\n",
      "[Epoch 0/200] [Batch 31/938] [D loss: 0.358494] [G loss: 0.730129]\n",
      "[Epoch 0/200] [Batch 32/938] [D loss: 0.359003] [G loss: 0.738047]\n",
      "[Epoch 0/200] [Batch 33/938] [D loss: 0.360868] [G loss: 0.711650]\n",
      "[Epoch 0/200] [Batch 34/938] [D loss: 0.365752] [G loss: 0.730335]\n",
      "[Epoch 0/200] [Batch 35/938] [D loss: 0.367100] [G loss: 0.736560]\n",
      "[Epoch 0/200] [Batch 36/938] [D loss: 0.371651] [G loss: 0.705758]\n",
      "[Epoch 0/200] [Batch 37/938] [D loss: 0.367800] [G loss: 0.738090]\n",
      "[Epoch 0/200] [Batch 38/938] [D loss: 0.362992] [G loss: 0.735602]\n",
      "[Epoch 0/200] [Batch 39/938] [D loss: 0.365892] [G loss: 0.749013]\n",
      "[Epoch 0/200] [Batch 40/938] [D loss: 0.368972] [G loss: 0.726886]\n",
      "[Epoch 0/200] [Batch 41/938] [D loss: 0.363461] [G loss: 0.800521]\n",
      "[Epoch 0/200] [Batch 42/938] [D loss: 0.378897] [G loss: 0.677585]\n",
      "[Epoch 0/200] [Batch 43/938] [D loss: 0.378575] [G loss: 0.816054]\n",
      "[Epoch 0/200] [Batch 44/938] [D loss: 0.389785] [G loss: 0.667371]\n",
      "[Epoch 0/200] [Batch 45/938] [D loss: 0.392113] [G loss: 0.762387]\n",
      "[Epoch 0/200] [Batch 46/938] [D loss: 0.407461] [G loss: 0.652288]\n",
      "[Epoch 0/200] [Batch 47/938] [D loss: 0.409350] [G loss: 0.731765]\n",
      "[Epoch 0/200] [Batch 48/938] [D loss: 0.421748] [G loss: 0.650266]\n",
      "[Epoch 0/200] [Batch 49/938] [D loss: 0.451445] [G loss: 0.717813]\n",
      "[Epoch 0/200] [Batch 50/938] [D loss: 0.490870] [G loss: 0.540263]\n",
      "[Epoch 0/200] [Batch 51/938] [D loss: 0.483442] [G loss: 0.780392]\n",
      "[Epoch 0/200] [Batch 52/938] [D loss: 0.567464] [G loss: 0.441072]\n",
      "[Epoch 0/200] [Batch 53/938] [D loss: 0.462450] [G loss: 0.722200]\n",
      "[Epoch 0/200] [Batch 54/938] [D loss: 0.462525] [G loss: 0.669394]\n",
      "[Epoch 0/200] [Batch 55/938] [D loss: 0.492599] [G loss: 0.619302]\n",
      "[Epoch 0/200] [Batch 56/938] [D loss: 0.490016] [G loss: 0.678636]\n",
      "[Epoch 0/200] [Batch 57/938] [D loss: 0.444386] [G loss: 0.674130]\n",
      "[Epoch 0/200] [Batch 58/938] [D loss: 0.456261] [G loss: 0.679025]\n",
      "[Epoch 0/200] [Batch 59/938] [D loss: 0.477846] [G loss: 0.769713]\n",
      "[Epoch 0/200] [Batch 60/938] [D loss: 0.572454] [G loss: 0.470440]\n",
      "[Epoch 0/200] [Batch 61/938] [D loss: 0.643771] [G loss: 1.104497]\n",
      "[Epoch 0/200] [Batch 62/938] [D loss: 0.755875] [G loss: 0.352830]\n",
      "[Epoch 0/200] [Batch 63/938] [D loss: 0.668992] [G loss: 0.412122]\n",
      "[Epoch 0/200] [Batch 64/938] [D loss: 0.621278] [G loss: 0.899439]\n",
      "[Epoch 0/200] [Batch 65/938] [D loss: 0.560983] [G loss: 0.533392]\n",
      "[Epoch 0/200] [Batch 66/938] [D loss: 0.600848] [G loss: 0.504659]\n",
      "[Epoch 0/200] [Batch 67/938] [D loss: 0.559164] [G loss: 0.794645]\n",
      "[Epoch 0/200] [Batch 68/938] [D loss: 0.591696] [G loss: 0.510728]\n",
      "[Epoch 0/200] [Batch 69/938] [D loss: 0.600176] [G loss: 0.516628]\n",
      "[Epoch 0/200] [Batch 70/938] [D loss: 0.549291] [G loss: 0.736098]\n",
      "[Epoch 0/200] [Batch 71/938] [D loss: 0.595972] [G loss: 0.516827]\n",
      "[Epoch 0/200] [Batch 72/938] [D loss: 0.526794] [G loss: 0.707292]\n",
      "[Epoch 0/200] [Batch 73/938] [D loss: 0.558613] [G loss: 0.656594]\n",
      "[Epoch 0/200] [Batch 74/938] [D loss: 0.523448] [G loss: 0.629745]\n",
      "[Epoch 0/200] [Batch 75/938] [D loss: 0.486460] [G loss: 0.678758]\n",
      "[Epoch 0/200] [Batch 76/938] [D loss: 0.482792] [G loss: 0.852809]\n",
      "[Epoch 0/200] [Batch 77/938] [D loss: 0.481286] [G loss: 0.597605]\n",
      "[Epoch 0/200] [Batch 78/938] [D loss: 0.412098] [G loss: 0.909103]\n",
      "[Epoch 0/200] [Batch 79/938] [D loss: 0.433080] [G loss: 0.859603]\n",
      "[Epoch 0/200] [Batch 80/938] [D loss: 0.469759] [G loss: 0.681937]\n",
      "[Epoch 0/200] [Batch 81/938] [D loss: 0.411534] [G loss: 1.010256]\n",
      "[Epoch 0/200] [Batch 82/938] [D loss: 0.441003] [G loss: 0.715448]\n",
      "[Epoch 0/200] [Batch 83/938] [D loss: 0.432674] [G loss: 0.920705]\n",
      "[Epoch 0/200] [Batch 84/938] [D loss: 0.476384] [G loss: 0.700402]\n",
      "[Epoch 0/200] [Batch 85/938] [D loss: 0.430574] [G loss: 0.938067]\n",
      "[Epoch 0/200] [Batch 86/938] [D loss: 0.555138] [G loss: 0.566350]\n",
      "[Epoch 0/200] [Batch 87/938] [D loss: 0.536400] [G loss: 0.920372]\n",
      "[Epoch 0/200] [Batch 88/938] [D loss: 0.638242] [G loss: 0.481420]\n",
      "[Epoch 0/200] [Batch 89/938] [D loss: 0.540098] [G loss: 0.698507]\n",
      "[Epoch 0/200] [Batch 90/938] [D loss: 0.547764] [G loss: 0.741176]\n",
      "[Epoch 0/200] [Batch 91/938] [D loss: 0.611066] [G loss: 0.558829]\n",
      "[Epoch 0/200] [Batch 92/938] [D loss: 0.591904] [G loss: 0.720053]\n",
      "[Epoch 0/200] [Batch 93/938] [D loss: 0.557644] [G loss: 0.583845]\n",
      "[Epoch 0/200] [Batch 94/938] [D loss: 0.501883] [G loss: 0.808932]\n",
      "[Epoch 0/200] [Batch 95/938] [D loss: 0.507562] [G loss: 0.668189]\n",
      "[Epoch 0/200] [Batch 96/938] [D loss: 0.539937] [G loss: 0.740510]\n",
      "[Epoch 0/200] [Batch 97/938] [D loss: 0.534483] [G loss: 0.668749]\n",
      "[Epoch 0/200] [Batch 98/938] [D loss: 0.495819] [G loss: 0.750560]\n",
      "[Epoch 0/200] [Batch 99/938] [D loss: 0.474209] [G loss: 0.721976]\n",
      "[Epoch 0/200] [Batch 100/938] [D loss: 0.404836] [G loss: 0.845269]\n",
      "[Epoch 0/200] [Batch 101/938] [D loss: 0.430676] [G loss: 0.997715]\n",
      "[Epoch 0/200] [Batch 102/938] [D loss: 0.473193] [G loss: 0.631444]\n",
      "[Epoch 0/200] [Batch 103/938] [D loss: 0.406747] [G loss: 1.053063]\n",
      "[Epoch 0/200] [Batch 104/938] [D loss: 0.380960] [G loss: 0.798765]\n",
      "[Epoch 0/200] [Batch 105/938] [D loss: 0.334924] [G loss: 1.118488]\n",
      "[Epoch 0/200] [Batch 106/938] [D loss: 0.342772] [G loss: 0.969857]\n",
      "[Epoch 0/200] [Batch 107/938] [D loss: 0.341707] [G loss: 0.995304]\n",
      "[Epoch 0/200] [Batch 108/938] [D loss: 0.347531] [G loss: 1.029975]\n",
      "[Epoch 0/200] [Batch 109/938] [D loss: 0.341915] [G loss: 0.973447]\n",
      "[Epoch 0/200] [Batch 110/938] [D loss: 0.350201] [G loss: 1.118124]\n",
      "[Epoch 0/200] [Batch 111/938] [D loss: 0.372093] [G loss: 0.912611]\n",
      "[Epoch 0/200] [Batch 112/938] [D loss: 0.355346] [G loss: 1.051633]\n",
      "[Epoch 0/200] [Batch 113/938] [D loss: 0.340847] [G loss: 1.031580]\n",
      "[Epoch 0/200] [Batch 114/938] [D loss: 0.342121] [G loss: 1.035429]\n",
      "[Epoch 0/200] [Batch 115/938] [D loss: 0.337780] [G loss: 1.016880]\n",
      "[Epoch 0/200] [Batch 116/938] [D loss: 0.327888] [G loss: 1.144839]\n",
      "[Epoch 0/200] [Batch 117/938] [D loss: 0.343684] [G loss: 1.073707]\n",
      "[Epoch 0/200] [Batch 118/938] [D loss: 0.366375] [G loss: 1.058391]\n",
      "[Epoch 0/200] [Batch 119/938] [D loss: 0.352781] [G loss: 0.940057]\n",
      "[Epoch 0/200] [Batch 120/938] [D loss: 0.392354] [G loss: 1.335120]\n",
      "[Epoch 0/200] [Batch 121/938] [D loss: 0.442899] [G loss: 0.800508]\n",
      "[Epoch 0/200] [Batch 122/938] [D loss: 0.333900] [G loss: 1.052400]\n",
      "[Epoch 0/200] [Batch 123/938] [D loss: 0.354778] [G loss: 1.310166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 124/938] [D loss: 0.396913] [G loss: 0.813594]\n",
      "[Epoch 0/200] [Batch 125/938] [D loss: 0.322828] [G loss: 1.120462]\n",
      "[Epoch 0/200] [Batch 126/938] [D loss: 0.328653] [G loss: 1.095278]\n",
      "[Epoch 0/200] [Batch 127/938] [D loss: 0.339832] [G loss: 0.922403]\n",
      "[Epoch 0/200] [Batch 128/938] [D loss: 0.338742] [G loss: 1.038011]\n",
      "[Epoch 0/200] [Batch 129/938] [D loss: 0.371686] [G loss: 0.994977]\n",
      "[Epoch 0/200] [Batch 130/938] [D loss: 0.385247] [G loss: 0.858124]\n",
      "[Epoch 0/200] [Batch 131/938] [D loss: 0.402340] [G loss: 1.086600]\n",
      "[Epoch 0/200] [Batch 132/938] [D loss: 0.411820] [G loss: 0.790037]\n",
      "[Epoch 0/200] [Batch 133/938] [D loss: 0.363549] [G loss: 1.013990]\n",
      "[Epoch 0/200] [Batch 134/938] [D loss: 0.371782] [G loss: 1.037991]\n",
      "[Epoch 0/200] [Batch 135/938] [D loss: 0.369179] [G loss: 0.924047]\n",
      "[Epoch 0/200] [Batch 136/938] [D loss: 0.367558] [G loss: 1.137531]\n",
      "[Epoch 0/200] [Batch 137/938] [D loss: 0.363116] [G loss: 0.877165]\n",
      "[Epoch 0/200] [Batch 138/938] [D loss: 0.327101] [G loss: 1.171713]\n",
      "[Epoch 0/200] [Batch 139/938] [D loss: 0.343852] [G loss: 1.062054]\n",
      "[Epoch 0/200] [Batch 140/938] [D loss: 0.336592] [G loss: 0.982528]\n",
      "[Epoch 0/200] [Batch 141/938] [D loss: 0.314395] [G loss: 1.207783]\n",
      "[Epoch 0/200] [Batch 142/938] [D loss: 0.333725] [G loss: 1.073959]\n",
      "[Epoch 0/200] [Batch 143/938] [D loss: 0.357464] [G loss: 0.909821]\n",
      "[Epoch 0/200] [Batch 144/938] [D loss: 0.337536] [G loss: 1.327292]\n",
      "[Epoch 0/200] [Batch 145/938] [D loss: 0.336576] [G loss: 0.943831]\n",
      "[Epoch 0/200] [Batch 146/938] [D loss: 0.306679] [G loss: 1.099437]\n",
      "[Epoch 0/200] [Batch 147/938] [D loss: 0.325637] [G loss: 1.330998]\n",
      "[Epoch 0/200] [Batch 148/938] [D loss: 0.363455] [G loss: 0.891933]\n",
      "[Epoch 0/200] [Batch 149/938] [D loss: 0.322258] [G loss: 1.308379]\n",
      "[Epoch 0/200] [Batch 150/938] [D loss: 0.325854] [G loss: 1.004535]\n",
      "[Epoch 0/200] [Batch 151/938] [D loss: 0.305871] [G loss: 1.231976]\n",
      "[Epoch 0/200] [Batch 152/938] [D loss: 0.310892] [G loss: 1.108524]\n",
      "[Epoch 0/200] [Batch 153/938] [D loss: 0.316199] [G loss: 1.079724]\n",
      "[Epoch 0/200] [Batch 154/938] [D loss: 0.334779] [G loss: 1.129240]\n",
      "[Epoch 0/200] [Batch 155/938] [D loss: 0.351671] [G loss: 0.941167]\n",
      "[Epoch 0/200] [Batch 156/938] [D loss: 0.370899] [G loss: 1.157204]\n",
      "[Epoch 0/200] [Batch 157/938] [D loss: 0.396834] [G loss: 0.775492]\n",
      "[Epoch 0/200] [Batch 158/938] [D loss: 0.394745] [G loss: 1.087197]\n",
      "[Epoch 0/200] [Batch 159/938] [D loss: 0.426838] [G loss: 0.789071]\n",
      "[Epoch 0/200] [Batch 160/938] [D loss: 0.416639] [G loss: 0.980024]\n",
      "[Epoch 0/200] [Batch 161/938] [D loss: 0.445598] [G loss: 0.732815]\n",
      "[Epoch 0/200] [Batch 162/938] [D loss: 0.456339] [G loss: 1.030267]\n",
      "[Epoch 0/200] [Batch 163/938] [D loss: 0.497349] [G loss: 0.624708]\n",
      "[Epoch 0/200] [Batch 164/938] [D loss: 0.518031] [G loss: 0.946063]\n",
      "[Epoch 0/200] [Batch 165/938] [D loss: 0.494520] [G loss: 0.644384]\n",
      "[Epoch 0/200] [Batch 166/938] [D loss: 0.492165] [G loss: 1.035730]\n",
      "[Epoch 0/200] [Batch 167/938] [D loss: 0.482224] [G loss: 0.648500]\n",
      "[Epoch 0/200] [Batch 168/938] [D loss: 0.456740] [G loss: 1.279148]\n",
      "[Epoch 0/200] [Batch 169/938] [D loss: 0.471182] [G loss: 0.629414]\n",
      "[Epoch 0/200] [Batch 170/938] [D loss: 0.458675] [G loss: 1.406765]\n",
      "[Epoch 0/200] [Batch 171/938] [D loss: 0.447705] [G loss: 0.649003]\n",
      "[Epoch 0/200] [Batch 172/938] [D loss: 0.363413] [G loss: 1.290925]\n",
      "[Epoch 0/200] [Batch 173/938] [D loss: 0.367416] [G loss: 1.021403]\n",
      "[Epoch 0/200] [Batch 174/938] [D loss: 0.395427] [G loss: 0.873544]\n",
      "[Epoch 0/200] [Batch 175/938] [D loss: 0.423726] [G loss: 1.044834]\n",
      "[Epoch 0/200] [Batch 176/938] [D loss: 0.480605] [G loss: 0.766111]\n",
      "[Epoch 0/200] [Batch 177/938] [D loss: 0.510026] [G loss: 0.797517]\n",
      "[Epoch 0/200] [Batch 178/938] [D loss: 0.591690] [G loss: 0.621564]\n",
      "[Epoch 0/200] [Batch 179/938] [D loss: 0.657332] [G loss: 0.827360]\n",
      "[Epoch 0/200] [Batch 180/938] [D loss: 0.728900] [G loss: 0.398465]\n",
      "[Epoch 0/200] [Batch 181/938] [D loss: 0.811604] [G loss: 1.072803]\n",
      "[Epoch 0/200] [Batch 182/938] [D loss: 0.932986] [G loss: 0.268000]\n",
      "[Epoch 0/200] [Batch 183/938] [D loss: 0.598193] [G loss: 0.787234]\n",
      "[Epoch 0/200] [Batch 184/938] [D loss: 0.606052] [G loss: 0.974088]\n",
      "[Epoch 0/200] [Batch 185/938] [D loss: 0.633646] [G loss: 0.454464]\n",
      "[Epoch 0/200] [Batch 186/938] [D loss: 0.515303] [G loss: 1.031594]\n",
      "[Epoch 0/200] [Batch 187/938] [D loss: 0.482374] [G loss: 0.814455]\n",
      "[Epoch 0/200] [Batch 188/938] [D loss: 0.505144] [G loss: 0.748345]\n",
      "[Epoch 0/200] [Batch 189/938] [D loss: 0.570777] [G loss: 0.816447]\n",
      "[Epoch 0/200] [Batch 190/938] [D loss: 0.615871] [G loss: 0.579830]\n",
      "[Epoch 0/200] [Batch 191/938] [D loss: 0.681168] [G loss: 0.748654]\n",
      "[Epoch 0/200] [Batch 192/938] [D loss: 0.735045] [G loss: 0.454489]\n",
      "[Epoch 0/200] [Batch 193/938] [D loss: 0.738380] [G loss: 0.646069]\n",
      "[Epoch 0/200] [Batch 194/938] [D loss: 0.755487] [G loss: 0.491068]\n",
      "[Epoch 0/200] [Batch 195/938] [D loss: 0.738565] [G loss: 0.567272]\n",
      "[Epoch 0/200] [Batch 196/938] [D loss: 0.735254] [G loss: 0.520437]\n",
      "[Epoch 0/200] [Batch 197/938] [D loss: 0.713078] [G loss: 0.581436]\n",
      "[Epoch 0/200] [Batch 198/938] [D loss: 0.682467] [G loss: 0.622918]\n",
      "[Epoch 0/200] [Batch 199/938] [D loss: 0.641119] [G loss: 0.566991]\n",
      "[Epoch 0/200] [Batch 200/938] [D loss: 0.658452] [G loss: 0.953459]\n",
      "[Epoch 0/200] [Batch 201/938] [D loss: 0.679448] [G loss: 0.396771]\n",
      "[Epoch 0/200] [Batch 202/938] [D loss: 0.601689] [G loss: 1.388692]\n",
      "[Epoch 0/200] [Batch 203/938] [D loss: 0.572597] [G loss: 0.481077]\n",
      "[Epoch 0/200] [Batch 204/938] [D loss: 0.484236] [G loss: 1.402389]\n",
      "[Epoch 0/200] [Batch 205/938] [D loss: 0.460692] [G loss: 0.691582]\n",
      "[Epoch 0/200] [Batch 206/938] [D loss: 0.449130] [G loss: 1.005681]\n",
      "[Epoch 0/200] [Batch 207/938] [D loss: 0.480118] [G loss: 0.831342]\n",
      "[Epoch 0/200] [Batch 208/938] [D loss: 0.531789] [G loss: 0.762585]\n",
      "[Epoch 0/200] [Batch 209/938] [D loss: 0.580582] [G loss: 0.712195]\n",
      "[Epoch 0/200] [Batch 210/938] [D loss: 0.637101] [G loss: 0.651422]\n",
      "[Epoch 0/200] [Batch 211/938] [D loss: 0.671644] [G loss: 0.600395]\n",
      "[Epoch 0/200] [Batch 212/938] [D loss: 0.722060] [G loss: 0.623224]\n",
      "[Epoch 0/200] [Batch 213/938] [D loss: 0.774880] [G loss: 0.447727]\n",
      "[Epoch 0/200] [Batch 214/938] [D loss: 0.747288] [G loss: 0.701964]\n",
      "[Epoch 0/200] [Batch 215/938] [D loss: 0.776885] [G loss: 0.375097]\n",
      "[Epoch 0/200] [Batch 216/938] [D loss: 0.696855] [G loss: 0.844489]\n",
      "[Epoch 0/200] [Batch 217/938] [D loss: 0.671736] [G loss: 0.471131]\n",
      "[Epoch 0/200] [Batch 218/938] [D loss: 0.582326] [G loss: 0.892177]\n",
      "[Epoch 0/200] [Batch 219/938] [D loss: 0.558099] [G loss: 0.678090]\n",
      "[Epoch 0/200] [Batch 220/938] [D loss: 0.512503] [G loss: 0.784899]\n",
      "[Epoch 0/200] [Batch 221/938] [D loss: 0.475312] [G loss: 0.888636]\n",
      "[Epoch 0/200] [Batch 222/938] [D loss: 0.450303] [G loss: 0.923742]\n",
      "[Epoch 0/200] [Batch 223/938] [D loss: 0.435907] [G loss: 0.946928]\n",
      "[Epoch 0/200] [Batch 224/938] [D loss: 0.437643] [G loss: 0.924742]\n",
      "[Epoch 0/200] [Batch 225/938] [D loss: 0.449265] [G loss: 0.953788]\n",
      "[Epoch 0/200] [Batch 226/938] [D loss: 0.458015] [G loss: 0.818275]\n",
      "[Epoch 0/200] [Batch 227/938] [D loss: 0.492092] [G loss: 1.067300]\n",
      "[Epoch 0/200] [Batch 228/938] [D loss: 0.588428] [G loss: 0.507464]\n",
      "[Epoch 0/200] [Batch 229/938] [D loss: 0.701313] [G loss: 1.505911]\n",
      "[Epoch 0/200] [Batch 230/938] [D loss: 0.929212] [G loss: 0.230311]\n",
      "[Epoch 0/200] [Batch 231/938] [D loss: 0.616346] [G loss: 1.082240]\n",
      "[Epoch 0/200] [Batch 232/938] [D loss: 0.597474] [G loss: 0.633012]\n",
      "[Epoch 0/200] [Batch 233/938] [D loss: 0.593931] [G loss: 0.666893]\n",
      "[Epoch 0/200] [Batch 234/938] [D loss: 0.596339] [G loss: 0.800302]\n",
      "[Epoch 0/200] [Batch 235/938] [D loss: 0.568447] [G loss: 0.655357]\n",
      "[Epoch 0/200] [Batch 236/938] [D loss: 0.537048] [G loss: 0.900764]\n",
      "[Epoch 0/200] [Batch 237/938] [D loss: 0.478444] [G loss: 0.698557]\n",
      "[Epoch 0/200] [Batch 238/938] [D loss: 0.447940] [G loss: 1.366678]\n",
      "[Epoch 0/200] [Batch 239/938] [D loss: 0.444742] [G loss: 0.657542]\n",
      "[Epoch 0/200] [Batch 240/938] [D loss: 0.335523] [G loss: 1.301594]\n",
      "[Epoch 0/200] [Batch 241/938] [D loss: 0.373670] [G loss: 1.213279]\n",
      "[Epoch 0/200] [Batch 242/938] [D loss: 0.443319] [G loss: 0.713040]\n",
      "[Epoch 0/200] [Batch 243/938] [D loss: 0.445000] [G loss: 1.310364]\n",
      "[Epoch 0/200] [Batch 244/938] [D loss: 0.517262] [G loss: 0.590047]\n",
      "[Epoch 0/200] [Batch 245/938] [D loss: 0.516286] [G loss: 1.292819]\n",
      "[Epoch 0/200] [Batch 246/938] [D loss: 0.581334] [G loss: 0.529269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 247/938] [D loss: 0.636257] [G loss: 1.019192]\n",
      "[Epoch 0/200] [Batch 248/938] [D loss: 0.680685] [G loss: 0.433722]\n",
      "[Epoch 0/200] [Batch 249/938] [D loss: 0.621521] [G loss: 0.922275]\n",
      "[Epoch 0/200] [Batch 250/938] [D loss: 0.610695] [G loss: 0.550358]\n",
      "[Epoch 0/200] [Batch 251/938] [D loss: 0.538945] [G loss: 0.900711]\n",
      "[Epoch 0/200] [Batch 252/938] [D loss: 0.513345] [G loss: 0.766006]\n",
      "[Epoch 0/200] [Batch 253/938] [D loss: 0.456191] [G loss: 0.914497]\n",
      "[Epoch 0/200] [Batch 254/938] [D loss: 0.442335] [G loss: 0.895324]\n",
      "[Epoch 0/200] [Batch 255/938] [D loss: 0.421176] [G loss: 0.866758]\n",
      "[Epoch 0/200] [Batch 256/938] [D loss: 0.404367] [G loss: 1.144556]\n",
      "[Epoch 0/200] [Batch 257/938] [D loss: 0.420488] [G loss: 0.756900]\n",
      "[Epoch 0/200] [Batch 258/938] [D loss: 0.478866] [G loss: 1.373667]\n",
      "[Epoch 0/200] [Batch 259/938] [D loss: 0.536227] [G loss: 0.511418]\n",
      "[Epoch 0/200] [Batch 260/938] [D loss: 0.478676] [G loss: 1.440284]\n",
      "[Epoch 0/200] [Batch 261/938] [D loss: 0.515656] [G loss: 0.554844]\n",
      "[Epoch 0/200] [Batch 262/938] [D loss: 0.477451] [G loss: 1.287593]\n",
      "[Epoch 0/200] [Batch 263/938] [D loss: 0.487656] [G loss: 0.621985]\n",
      "[Epoch 0/200] [Batch 264/938] [D loss: 0.503335] [G loss: 1.166343]\n",
      "[Epoch 0/200] [Batch 265/938] [D loss: 0.531502] [G loss: 0.555117]\n",
      "[Epoch 0/200] [Batch 266/938] [D loss: 0.524179] [G loss: 1.310828]\n",
      "[Epoch 0/200] [Batch 267/938] [D loss: 0.571553] [G loss: 0.467205]\n",
      "[Epoch 0/200] [Batch 268/938] [D loss: 0.474421] [G loss: 1.395096]\n",
      "[Epoch 0/200] [Batch 269/938] [D loss: 0.470730] [G loss: 0.657938]\n",
      "[Epoch 0/200] [Batch 270/938] [D loss: 0.424150] [G loss: 1.064158]\n",
      "[Epoch 0/200] [Batch 271/938] [D loss: 0.425603] [G loss: 0.917145]\n",
      "[Epoch 0/200] [Batch 272/938] [D loss: 0.469484] [G loss: 0.915533]\n",
      "[Epoch 0/200] [Batch 273/938] [D loss: 0.447003] [G loss: 0.776837]\n",
      "[Epoch 0/200] [Batch 274/938] [D loss: 0.504937] [G loss: 1.260581]\n",
      "[Epoch 0/200] [Batch 275/938] [D loss: 0.599140] [G loss: 0.418085]\n",
      "[Epoch 0/200] [Batch 276/938] [D loss: 0.544297] [G loss: 1.627701]\n",
      "[Epoch 0/200] [Batch 277/938] [D loss: 0.559770] [G loss: 0.482956]\n",
      "[Epoch 0/200] [Batch 278/938] [D loss: 0.438532] [G loss: 1.135229]\n",
      "[Epoch 0/200] [Batch 279/938] [D loss: 0.431565] [G loss: 0.877019]\n",
      "[Epoch 0/200] [Batch 280/938] [D loss: 0.425532] [G loss: 0.912702]\n",
      "[Epoch 0/200] [Batch 281/938] [D loss: 0.475792] [G loss: 0.988659]\n",
      "[Epoch 0/200] [Batch 282/938] [D loss: 0.492218] [G loss: 0.659401]\n",
      "[Epoch 0/200] [Batch 283/938] [D loss: 0.487285] [G loss: 1.258919]\n",
      "[Epoch 0/200] [Batch 284/938] [D loss: 0.544217] [G loss: 0.517949]\n",
      "[Epoch 0/200] [Batch 285/938] [D loss: 0.484730] [G loss: 1.334938]\n",
      "[Epoch 0/200] [Batch 286/938] [D loss: 0.479103] [G loss: 0.627077]\n",
      "[Epoch 0/200] [Batch 287/938] [D loss: 0.415307] [G loss: 1.273227]\n",
      "[Epoch 0/200] [Batch 288/938] [D loss: 0.410356] [G loss: 0.770167]\n",
      "[Epoch 0/200] [Batch 289/938] [D loss: 0.359530] [G loss: 1.268127]\n",
      "[Epoch 0/200] [Batch 290/938] [D loss: 0.375232] [G loss: 0.934167]\n",
      "[Epoch 0/200] [Batch 291/938] [D loss: 0.374914] [G loss: 1.068293]\n",
      "[Epoch 0/200] [Batch 292/938] [D loss: 0.358124] [G loss: 1.010640]\n",
      "[Epoch 0/200] [Batch 293/938] [D loss: 0.399703] [G loss: 1.083378]\n",
      "[Epoch 0/200] [Batch 294/938] [D loss: 0.429632] [G loss: 0.726131]\n",
      "[Epoch 0/200] [Batch 295/938] [D loss: 0.450021] [G loss: 1.487685]\n",
      "[Epoch 0/200] [Batch 296/938] [D loss: 0.569406] [G loss: 0.441317]\n",
      "[Epoch 0/200] [Batch 297/938] [D loss: 0.450123] [G loss: 1.634389]\n",
      "[Epoch 0/200] [Batch 298/938] [D loss: 0.425557] [G loss: 0.647058]\n",
      "[Epoch 0/200] [Batch 299/938] [D loss: 0.364380] [G loss: 1.345503]\n",
      "[Epoch 0/200] [Batch 300/938] [D loss: 0.360200] [G loss: 0.921106]\n",
      "[Epoch 0/200] [Batch 301/938] [D loss: 0.382852] [G loss: 1.105013]\n",
      "[Epoch 0/200] [Batch 302/938] [D loss: 0.381348] [G loss: 0.928342]\n",
      "[Epoch 0/200] [Batch 303/938] [D loss: 0.398796] [G loss: 1.147897]\n",
      "[Epoch 0/200] [Batch 304/938] [D loss: 0.432802] [G loss: 0.719512]\n",
      "[Epoch 0/200] [Batch 305/938] [D loss: 0.390599] [G loss: 1.395095]\n",
      "[Epoch 0/200] [Batch 306/938] [D loss: 0.458560] [G loss: 0.659363]\n",
      "[Epoch 0/200] [Batch 307/938] [D loss: 0.451718] [G loss: 1.515926]\n",
      "[Epoch 0/200] [Batch 308/938] [D loss: 0.571700] [G loss: 0.435418]\n",
      "[Epoch 0/200] [Batch 309/938] [D loss: 0.490969] [G loss: 1.776479]\n",
      "[Epoch 0/200] [Batch 310/938] [D loss: 0.506260] [G loss: 0.525774]\n",
      "[Epoch 0/200] [Batch 311/938] [D loss: 0.320518] [G loss: 1.471932]\n",
      "[Epoch 0/200] [Batch 312/938] [D loss: 0.355053] [G loss: 1.137949]\n",
      "[Epoch 0/200] [Batch 313/938] [D loss: 0.423274] [G loss: 0.737477]\n",
      "[Epoch 0/200] [Batch 314/938] [D loss: 0.471677] [G loss: 1.647561]\n",
      "[Epoch 0/200] [Batch 315/938] [D loss: 0.596925] [G loss: 0.411392]\n",
      "[Epoch 0/200] [Batch 316/938] [D loss: 0.377783] [G loss: 1.644445]\n",
      "[Epoch 0/200] [Batch 317/938] [D loss: 0.357652] [G loss: 0.985670]\n",
      "[Epoch 0/200] [Batch 318/938] [D loss: 0.343517] [G loss: 0.937151]\n",
      "[Epoch 0/200] [Batch 319/938] [D loss: 0.349052] [G loss: 1.549680]\n",
      "[Epoch 0/200] [Batch 320/938] [D loss: 0.422658] [G loss: 0.686738]\n",
      "[Epoch 0/200] [Batch 321/938] [D loss: 0.393792] [G loss: 1.680755]\n",
      "[Epoch 0/200] [Batch 322/938] [D loss: 0.442468] [G loss: 0.648906]\n",
      "[Epoch 0/200] [Batch 323/938] [D loss: 0.330901] [G loss: 1.541877]\n",
      "[Epoch 0/200] [Batch 324/938] [D loss: 0.360859] [G loss: 0.999591]\n",
      "[Epoch 0/200] [Batch 325/938] [D loss: 0.346266] [G loss: 1.033370]\n",
      "[Epoch 0/200] [Batch 326/938] [D loss: 0.364807] [G loss: 1.341219]\n",
      "[Epoch 0/200] [Batch 327/938] [D loss: 0.425056] [G loss: 0.733690]\n",
      "[Epoch 0/200] [Batch 328/938] [D loss: 0.363707] [G loss: 1.756255]\n",
      "[Epoch 0/200] [Batch 329/938] [D loss: 0.474092] [G loss: 0.577265]\n",
      "[Epoch 0/200] [Batch 330/938] [D loss: 0.473307] [G loss: 1.963171]\n",
      "[Epoch 0/200] [Batch 331/938] [D loss: 0.622482] [G loss: 0.381348]\n",
      "[Epoch 0/200] [Batch 332/938] [D loss: 0.346094] [G loss: 2.078543]\n",
      "[Epoch 0/200] [Batch 333/938] [D loss: 0.318418] [G loss: 0.965162]\n",
      "[Epoch 0/200] [Batch 334/938] [D loss: 0.309041] [G loss: 1.103643]\n",
      "[Epoch 0/200] [Batch 335/938] [D loss: 0.339313] [G loss: 1.278201]\n",
      "[Epoch 0/200] [Batch 336/938] [D loss: 0.411331] [G loss: 0.801413]\n",
      "[Epoch 0/200] [Batch 337/938] [D loss: 0.377880] [G loss: 1.342980]\n",
      "[Epoch 0/200] [Batch 338/938] [D loss: 0.480227] [G loss: 0.627572]\n",
      "[Epoch 0/200] [Batch 339/938] [D loss: 0.423483] [G loss: 1.395614]\n",
      "[Epoch 0/200] [Batch 340/938] [D loss: 0.453447] [G loss: 0.632831]\n",
      "[Epoch 0/200] [Batch 341/938] [D loss: 0.355807] [G loss: 1.514488]\n",
      "[Epoch 0/200] [Batch 342/938] [D loss: 0.375868] [G loss: 0.787256]\n",
      "[Epoch 0/200] [Batch 343/938] [D loss: 0.376167] [G loss: 1.321001]\n",
      "[Epoch 0/200] [Batch 344/938] [D loss: 0.390461] [G loss: 0.805320]\n",
      "[Epoch 0/200] [Batch 345/938] [D loss: 0.328623] [G loss: 1.262841]\n",
      "[Epoch 0/200] [Batch 346/938] [D loss: 0.357801] [G loss: 1.008170]\n",
      "[Epoch 0/200] [Batch 347/938] [D loss: 0.337079] [G loss: 1.093406]\n",
      "[Epoch 0/200] [Batch 348/938] [D loss: 0.332308] [G loss: 1.145388]\n",
      "[Epoch 0/200] [Batch 349/938] [D loss: 0.346267] [G loss: 1.071304]\n",
      "[Epoch 0/200] [Batch 350/938] [D loss: 0.309241] [G loss: 1.084447]\n",
      "[Epoch 0/200] [Batch 351/938] [D loss: 0.275791] [G loss: 1.199005]\n",
      "[Epoch 0/200] [Batch 352/938] [D loss: 0.256206] [G loss: 1.278176]\n",
      "[Epoch 0/200] [Batch 353/938] [D loss: 0.278594] [G loss: 1.243733]\n",
      "[Epoch 0/200] [Batch 354/938] [D loss: 0.248999] [G loss: 1.115905]\n",
      "[Epoch 0/200] [Batch 355/938] [D loss: 0.271824] [G loss: 1.588942]\n",
      "[Epoch 0/200] [Batch 356/938] [D loss: 0.282992] [G loss: 0.978076]\n",
      "[Epoch 0/200] [Batch 357/938] [D loss: 0.255419] [G loss: 1.486608]\n",
      "[Epoch 0/200] [Batch 358/938] [D loss: 0.252483] [G loss: 1.139875]\n",
      "[Epoch 0/200] [Batch 359/938] [D loss: 0.249535] [G loss: 1.255471]\n",
      "[Epoch 0/200] [Batch 360/938] [D loss: 0.253780] [G loss: 1.206800]\n",
      "[Epoch 0/200] [Batch 361/938] [D loss: 0.251355] [G loss: 1.190367]\n",
      "[Epoch 0/200] [Batch 362/938] [D loss: 0.278640] [G loss: 1.145866]\n",
      "[Epoch 0/200] [Batch 363/938] [D loss: 0.264476] [G loss: 1.106717]\n",
      "[Epoch 0/200] [Batch 364/938] [D loss: 0.282701] [G loss: 1.323799]\n",
      "[Epoch 0/200] [Batch 365/938] [D loss: 0.350420] [G loss: 0.812978]\n",
      "[Epoch 0/200] [Batch 366/938] [D loss: 0.389384] [G loss: 1.724920]\n",
      "[Epoch 0/200] [Batch 367/938] [D loss: 0.549181] [G loss: 0.612673]\n",
      "[Epoch 0/200] [Batch 368/938] [D loss: 0.304330] [G loss: 1.586368]\n",
      "[Epoch 0/200] [Batch 369/938] [D loss: 0.296132] [G loss: 1.087003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 370/938] [D loss: 0.337967] [G loss: 0.830118]\n",
      "[Epoch 0/200] [Batch 371/938] [D loss: 0.407885] [G loss: 1.418488]\n",
      "[Epoch 0/200] [Batch 372/938] [D loss: 0.479419] [G loss: 0.663531]\n",
      "[Epoch 0/200] [Batch 373/938] [D loss: 0.379963] [G loss: 1.131680]\n",
      "[Epoch 0/200] [Batch 374/938] [D loss: 0.395456] [G loss: 0.877015]\n",
      "[Epoch 0/200] [Batch 375/938] [D loss: 0.404186] [G loss: 0.830393]\n",
      "[Epoch 0/200] [Batch 376/938] [D loss: 0.448899] [G loss: 1.027930]\n",
      "[Epoch 0/200] [Batch 377/938] [D loss: 0.484874] [G loss: 0.621787]\n",
      "[Epoch 0/200] [Batch 378/938] [D loss: 0.469776] [G loss: 1.201019]\n",
      "[Epoch 0/200] [Batch 379/938] [D loss: 0.509011] [G loss: 0.568404]\n",
      "[Epoch 0/200] [Batch 380/938] [D loss: 0.400680] [G loss: 1.112690]\n",
      "[Epoch 0/200] [Batch 381/938] [D loss: 0.419729] [G loss: 0.943965]\n",
      "[Epoch 0/200] [Batch 382/938] [D loss: 0.400793] [G loss: 0.792278]\n",
      "[Epoch 0/200] [Batch 383/938] [D loss: 0.323846] [G loss: 1.200377]\n",
      "[Epoch 0/200] [Batch 384/938] [D loss: 0.300271] [G loss: 1.226056]\n",
      "[Epoch 0/200] [Batch 385/938] [D loss: 0.315583] [G loss: 1.073163]\n",
      "[Epoch 0/200] [Batch 386/938] [D loss: 0.296569] [G loss: 1.191364]\n",
      "[Epoch 0/200] [Batch 387/938] [D loss: 0.298130] [G loss: 1.255075]\n",
      "[Epoch 0/200] [Batch 388/938] [D loss: 0.298579] [G loss: 1.106076]\n",
      "[Epoch 0/200] [Batch 389/938] [D loss: 0.300571] [G loss: 1.471843]\n",
      "[Epoch 0/200] [Batch 390/938] [D loss: 0.341895] [G loss: 1.003930]\n",
      "[Epoch 0/200] [Batch 391/938] [D loss: 0.357590] [G loss: 1.335228]\n",
      "[Epoch 0/200] [Batch 392/938] [D loss: 0.385356] [G loss: 0.928627]\n",
      "[Epoch 0/200] [Batch 393/938] [D loss: 0.464100] [G loss: 1.335516]\n",
      "[Epoch 0/200] [Batch 394/938] [D loss: 0.624162] [G loss: 0.486623]\n",
      "[Epoch 0/200] [Batch 395/938] [D loss: 0.619660] [G loss: 2.200321]\n",
      "[Epoch 0/200] [Batch 396/938] [D loss: 0.797859] [G loss: 0.323153]\n",
      "[Epoch 0/200] [Batch 397/938] [D loss: 0.366685] [G loss: 1.145173]\n",
      "[Epoch 0/200] [Batch 398/938] [D loss: 0.468786] [G loss: 1.984658]\n",
      "[Epoch 0/200] [Batch 399/938] [D loss: 0.609052] [G loss: 0.471727]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.378204] [G loss: 1.168060]\n",
      "[Epoch 0/200] [Batch 401/938] [D loss: 0.448683] [G loss: 1.499208]\n",
      "[Epoch 0/200] [Batch 402/938] [D loss: 0.520126] [G loss: 0.608562]\n",
      "[Epoch 0/200] [Batch 403/938] [D loss: 0.405683] [G loss: 1.284921]\n",
      "[Epoch 0/200] [Batch 404/938] [D loss: 0.423279] [G loss: 1.202941]\n",
      "[Epoch 0/200] [Batch 405/938] [D loss: 0.442057] [G loss: 0.830985]\n",
      "[Epoch 0/200] [Batch 406/938] [D loss: 0.417077] [G loss: 1.279808]\n",
      "[Epoch 0/200] [Batch 407/938] [D loss: 0.439907] [G loss: 0.874078]\n",
      "[Epoch 0/200] [Batch 408/938] [D loss: 0.457975] [G loss: 1.314937]\n",
      "[Epoch 0/200] [Batch 409/938] [D loss: 0.511963] [G loss: 0.693047]\n",
      "[Epoch 0/200] [Batch 410/938] [D loss: 0.477924] [G loss: 1.626233]\n",
      "[Epoch 0/200] [Batch 411/938] [D loss: 0.561111] [G loss: 0.583313]\n",
      "[Epoch 0/200] [Batch 412/938] [D loss: 0.459883] [G loss: 1.481049]\n",
      "[Epoch 0/200] [Batch 413/938] [D loss: 0.429391] [G loss: 0.841403]\n",
      "[Epoch 0/200] [Batch 414/938] [D loss: 0.427157] [G loss: 1.367094]\n",
      "[Epoch 0/200] [Batch 415/938] [D loss: 0.445858] [G loss: 0.805372]\n",
      "[Epoch 0/200] [Batch 416/938] [D loss: 0.467278] [G loss: 1.410550]\n",
      "[Epoch 0/200] [Batch 417/938] [D loss: 0.634479] [G loss: 0.433877]\n",
      "[Epoch 0/200] [Batch 418/938] [D loss: 0.600720] [G loss: 1.973468]\n",
      "[Epoch 0/200] [Batch 419/938] [D loss: 0.681450] [G loss: 0.382042]\n",
      "[Epoch 0/200] [Batch 420/938] [D loss: 0.428556] [G loss: 1.378353]\n",
      "[Epoch 0/200] [Batch 421/938] [D loss: 0.401699] [G loss: 1.125673]\n",
      "[Epoch 0/200] [Batch 422/938] [D loss: 0.438461] [G loss: 0.819443]\n",
      "[Epoch 0/200] [Batch 423/938] [D loss: 0.462217] [G loss: 1.414298]\n",
      "[Epoch 0/200] [Batch 424/938] [D loss: 0.582863] [G loss: 0.473423]\n",
      "[Epoch 0/200] [Batch 425/938] [D loss: 0.628067] [G loss: 1.944173]\n",
      "[Epoch 0/200] [Batch 426/938] [D loss: 0.689294] [G loss: 0.374266]\n",
      "[Epoch 0/200] [Batch 427/938] [D loss: 0.492374] [G loss: 1.309879]\n",
      "[Epoch 0/200] [Batch 428/938] [D loss: 0.458255] [G loss: 0.922362]\n",
      "[Epoch 0/200] [Batch 429/938] [D loss: 0.479740] [G loss: 0.804892]\n",
      "[Epoch 0/200] [Batch 430/938] [D loss: 0.515737] [G loss: 1.239408]\n",
      "[Epoch 0/200] [Batch 431/938] [D loss: 0.594089] [G loss: 0.487432]\n",
      "[Epoch 0/200] [Batch 432/938] [D loss: 0.622587] [G loss: 1.749393]\n",
      "[Epoch 0/200] [Batch 433/938] [D loss: 0.644689] [G loss: 0.432771]\n",
      "[Epoch 0/200] [Batch 434/938] [D loss: 0.468209] [G loss: 1.566424]\n",
      "[Epoch 0/200] [Batch 435/938] [D loss: 0.426465] [G loss: 1.066817]\n",
      "[Epoch 0/200] [Batch 436/938] [D loss: 0.448278] [G loss: 0.837788]\n",
      "[Epoch 0/200] [Batch 437/938] [D loss: 0.448845] [G loss: 1.412250]\n",
      "[Epoch 0/200] [Batch 438/938] [D loss: 0.460044] [G loss: 0.713210]\n",
      "[Epoch 0/200] [Batch 439/938] [D loss: 0.454624] [G loss: 1.447687]\n",
      "[Epoch 0/200] [Batch 440/938] [D loss: 0.476959] [G loss: 0.667622]\n",
      "[Epoch 0/200] [Batch 441/938] [D loss: 0.500868] [G loss: 1.628793]\n",
      "[Epoch 0/200] [Batch 442/938] [D loss: 0.563105] [G loss: 0.535086]\n",
      "[Epoch 0/200] [Batch 443/938] [D loss: 0.461874] [G loss: 1.632917]\n",
      "[Epoch 0/200] [Batch 444/938] [D loss: 0.465856] [G loss: 0.800455]\n",
      "[Epoch 0/200] [Batch 445/938] [D loss: 0.425740] [G loss: 1.117898]\n",
      "[Epoch 0/200] [Batch 446/938] [D loss: 0.438101] [G loss: 1.049590]\n",
      "[Epoch 0/200] [Batch 447/938] [D loss: 0.463373] [G loss: 0.902744]\n",
      "[Epoch 0/200] [Batch 448/938] [D loss: 0.471724] [G loss: 1.016604]\n",
      "[Epoch 0/200] [Batch 449/938] [D loss: 0.481104] [G loss: 0.780660]\n",
      "[Epoch 0/200] [Batch 450/938] [D loss: 0.472232] [G loss: 1.321104]\n",
      "[Epoch 0/200] [Batch 451/938] [D loss: 0.515610] [G loss: 0.585666]\n",
      "[Epoch 0/200] [Batch 452/938] [D loss: 0.482582] [G loss: 1.653385]\n",
      "[Epoch 0/200] [Batch 453/938] [D loss: 0.553622] [G loss: 0.506876]\n",
      "[Epoch 0/200] [Batch 454/938] [D loss: 0.529512] [G loss: 1.701783]\n",
      "[Epoch 0/200] [Batch 455/938] [D loss: 0.554883] [G loss: 0.501810]\n",
      "[Epoch 0/200] [Batch 456/938] [D loss: 0.406017] [G loss: 1.522656]\n",
      "[Epoch 0/200] [Batch 457/938] [D loss: 0.386455] [G loss: 1.005709]\n",
      "[Epoch 0/200] [Batch 458/938] [D loss: 0.369956] [G loss: 1.070694]\n",
      "[Epoch 0/200] [Batch 459/938] [D loss: 0.420363] [G loss: 1.213173]\n",
      "[Epoch 0/200] [Batch 460/938] [D loss: 0.440595] [G loss: 0.813610]\n",
      "[Epoch 0/200] [Batch 461/938] [D loss: 0.457116] [G loss: 1.397573]\n",
      "[Epoch 0/200] [Batch 462/938] [D loss: 0.505394] [G loss: 0.624810]\n",
      "[Epoch 0/200] [Batch 463/938] [D loss: 0.482431] [G loss: 1.712695]\n",
      "[Epoch 0/200] [Batch 464/938] [D loss: 0.539701] [G loss: 0.596608]\n",
      "[Epoch 0/200] [Batch 465/938] [D loss: 0.466802] [G loss: 1.581081]\n",
      "[Epoch 0/200] [Batch 466/938] [D loss: 0.486141] [G loss: 0.674520]\n",
      "[Epoch 0/200] [Batch 467/938] [D loss: 0.413277] [G loss: 1.503571]\n",
      "[Epoch 0/200] [Batch 468/938] [D loss: 0.439551] [G loss: 0.755178]\n",
      "[Epoch 0/200] [Batch 469/938] [D loss: 0.462589] [G loss: 1.444425]\n",
      "[Epoch 0/200] [Batch 470/938] [D loss: 0.566700] [G loss: 0.488681]\n",
      "[Epoch 0/200] [Batch 471/938] [D loss: 0.532475] [G loss: 2.046113]\n",
      "[Epoch 0/200] [Batch 472/938] [D loss: 0.629687] [G loss: 0.428697]\n",
      "[Epoch 0/200] [Batch 473/938] [D loss: 0.429218] [G loss: 1.600499]\n",
      "[Epoch 0/200] [Batch 474/938] [D loss: 0.419769] [G loss: 0.897714]\n",
      "[Epoch 0/200] [Batch 475/938] [D loss: 0.404987] [G loss: 1.114367]\n",
      "[Epoch 0/200] [Batch 476/938] [D loss: 0.460945] [G loss: 0.947961]\n",
      "[Epoch 0/200] [Batch 477/938] [D loss: 0.458926] [G loss: 0.946270]\n",
      "[Epoch 0/200] [Batch 478/938] [D loss: 0.471563] [G loss: 1.091220]\n",
      "[Epoch 0/200] [Batch 479/938] [D loss: 0.488097] [G loss: 0.701461]\n",
      "[Epoch 0/200] [Batch 480/938] [D loss: 0.513812] [G loss: 1.643046]\n",
      "[Epoch 0/200] [Batch 481/938] [D loss: 0.620089] [G loss: 0.455049]\n",
      "[Epoch 0/200] [Batch 482/938] [D loss: 0.468698] [G loss: 1.735159]\n",
      "[Epoch 0/200] [Batch 483/938] [D loss: 0.429566] [G loss: 0.779739]\n",
      "[Epoch 0/200] [Batch 484/938] [D loss: 0.341157] [G loss: 1.268708]\n",
      "[Epoch 0/200] [Batch 485/938] [D loss: 0.378405] [G loss: 1.333105]\n",
      "[Epoch 0/200] [Batch 486/938] [D loss: 0.394556] [G loss: 0.815953]\n",
      "[Epoch 0/200] [Batch 487/938] [D loss: 0.419545] [G loss: 1.654626]\n",
      "[Epoch 0/200] [Batch 488/938] [D loss: 0.479272] [G loss: 0.596478]\n",
      "[Epoch 0/200] [Batch 489/938] [D loss: 0.415609] [G loss: 1.879166]\n",
      "[Epoch 0/200] [Batch 490/938] [D loss: 0.413528] [G loss: 0.785555]\n",
      "[Epoch 0/200] [Batch 491/938] [D loss: 0.307524] [G loss: 1.305433]\n",
      "[Epoch 0/200] [Batch 492/938] [D loss: 0.413114] [G loss: 1.386219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 493/938] [D loss: 0.568862] [G loss: 0.460026]\n",
      "[Epoch 0/200] [Batch 494/938] [D loss: 0.647703] [G loss: 2.094856]\n",
      "[Epoch 0/200] [Batch 495/938] [D loss: 0.665535] [G loss: 0.378429]\n",
      "[Epoch 0/200] [Batch 496/938] [D loss: 0.361404] [G loss: 1.413469]\n",
      "[Epoch 0/200] [Batch 497/938] [D loss: 0.343862] [G loss: 1.455541]\n",
      "[Epoch 0/200] [Batch 498/938] [D loss: 0.357597] [G loss: 0.863522]\n",
      "[Epoch 0/200] [Batch 499/938] [D loss: 0.332743] [G loss: 1.540942]\n",
      "[Epoch 0/200] [Batch 500/938] [D loss: 0.358631] [G loss: 0.994824]\n",
      "[Epoch 0/200] [Batch 501/938] [D loss: 0.335342] [G loss: 1.245835]\n",
      "[Epoch 0/200] [Batch 502/938] [D loss: 0.357750] [G loss: 1.152312]\n",
      "[Epoch 0/200] [Batch 503/938] [D loss: 0.347102] [G loss: 1.062285]\n",
      "[Epoch 0/200] [Batch 504/938] [D loss: 0.388236] [G loss: 1.296670]\n",
      "[Epoch 0/200] [Batch 505/938] [D loss: 0.421370] [G loss: 0.753871]\n",
      "[Epoch 0/200] [Batch 506/938] [D loss: 0.449286] [G loss: 1.721905]\n",
      "[Epoch 0/200] [Batch 507/938] [D loss: 0.523782] [G loss: 0.540279]\n",
      "[Epoch 0/200] [Batch 508/938] [D loss: 0.393164] [G loss: 1.674316]\n",
      "[Epoch 0/200] [Batch 509/938] [D loss: 0.360544] [G loss: 0.942225]\n",
      "[Epoch 0/200] [Batch 510/938] [D loss: 0.316274] [G loss: 1.236535]\n",
      "[Epoch 0/200] [Batch 511/938] [D loss: 0.308839] [G loss: 1.208884]\n",
      "[Epoch 0/200] [Batch 512/938] [D loss: 0.347014] [G loss: 1.173136]\n",
      "[Epoch 0/200] [Batch 513/938] [D loss: 0.364995] [G loss: 1.018422]\n",
      "[Epoch 0/200] [Batch 514/938] [D loss: 0.371904] [G loss: 1.306296]\n",
      "[Epoch 0/200] [Batch 515/938] [D loss: 0.438846] [G loss: 0.699393]\n",
      "[Epoch 0/200] [Batch 516/938] [D loss: 0.495020] [G loss: 1.957083]\n",
      "[Epoch 0/200] [Batch 517/938] [D loss: 0.579531] [G loss: 0.463612]\n",
      "[Epoch 0/200] [Batch 518/938] [D loss: 0.415237] [G loss: 1.699978]\n",
      "[Epoch 0/200] [Batch 519/938] [D loss: 0.384164] [G loss: 0.746458]\n",
      "[Epoch 0/200] [Batch 520/938] [D loss: 0.332031] [G loss: 1.547287]\n",
      "[Epoch 0/200] [Batch 521/938] [D loss: 0.301148] [G loss: 1.018257]\n",
      "[Epoch 0/200] [Batch 522/938] [D loss: 0.300043] [G loss: 1.376887]\n",
      "[Epoch 0/200] [Batch 523/938] [D loss: 0.302105] [G loss: 1.127401]\n",
      "[Epoch 0/200] [Batch 524/938] [D loss: 0.334921] [G loss: 1.275926]\n",
      "[Epoch 0/200] [Batch 525/938] [D loss: 0.353345] [G loss: 0.921131]\n",
      "[Epoch 0/200] [Batch 526/938] [D loss: 0.342934] [G loss: 1.575883]\n",
      "[Epoch 0/200] [Batch 527/938] [D loss: 0.359336] [G loss: 0.812952]\n",
      "[Epoch 0/200] [Batch 528/938] [D loss: 0.405759] [G loss: 1.832869]\n",
      "[Epoch 0/200] [Batch 529/938] [D loss: 0.486118] [G loss: 0.572708]\n",
      "[Epoch 0/200] [Batch 530/938] [D loss: 0.308684] [G loss: 2.079016]\n",
      "[Epoch 0/200] [Batch 531/938] [D loss: 0.245687] [G loss: 1.243443]\n",
      "[Epoch 0/200] [Batch 532/938] [D loss: 0.274978] [G loss: 1.086891]\n",
      "[Epoch 0/200] [Batch 533/938] [D loss: 0.277149] [G loss: 1.555741]\n",
      "[Epoch 0/200] [Batch 534/938] [D loss: 0.347264] [G loss: 0.925527]\n",
      "[Epoch 0/200] [Batch 535/938] [D loss: 0.318743] [G loss: 1.490229]\n",
      "[Epoch 0/200] [Batch 536/938] [D loss: 0.342331] [G loss: 0.856500]\n",
      "[Epoch 0/200] [Batch 537/938] [D loss: 0.426757] [G loss: 1.847136]\n",
      "[Epoch 0/200] [Batch 538/938] [D loss: 0.618729] [G loss: 0.444887]\n",
      "[Epoch 0/200] [Batch 539/938] [D loss: 0.355298] [G loss: 1.856949]\n",
      "[Epoch 0/200] [Batch 540/938] [D loss: 0.289068] [G loss: 1.071589]\n",
      "[Epoch 0/200] [Batch 541/938] [D loss: 0.294688] [G loss: 1.158916]\n",
      "[Epoch 0/200] [Batch 542/938] [D loss: 0.277679] [G loss: 1.406428]\n",
      "[Epoch 0/200] [Batch 543/938] [D loss: 0.347127] [G loss: 1.018333]\n",
      "[Epoch 0/200] [Batch 544/938] [D loss: 0.346321] [G loss: 1.212143]\n",
      "[Epoch 0/200] [Batch 545/938] [D loss: 0.378522] [G loss: 0.926327]\n",
      "[Epoch 0/200] [Batch 546/938] [D loss: 0.349880] [G loss: 1.305522]\n",
      "[Epoch 0/200] [Batch 547/938] [D loss: 0.381208] [G loss: 0.849026]\n",
      "[Epoch 0/200] [Batch 548/938] [D loss: 0.362335] [G loss: 1.626034]\n",
      "[Epoch 0/200] [Batch 549/938] [D loss: 0.438466] [G loss: 0.691111]\n",
      "[Epoch 0/200] [Batch 550/938] [D loss: 0.496690] [G loss: 1.984877]\n",
      "[Epoch 0/200] [Batch 551/938] [D loss: 0.651507] [G loss: 0.418812]\n",
      "[Epoch 0/200] [Batch 552/938] [D loss: 0.275926] [G loss: 1.631578]\n",
      "[Epoch 0/200] [Batch 553/938] [D loss: 0.242171] [G loss: 1.620339]\n",
      "[Epoch 0/200] [Batch 554/938] [D loss: 0.289074] [G loss: 1.037655]\n",
      "[Epoch 0/200] [Batch 555/938] [D loss: 0.271488] [G loss: 1.514334]\n",
      "[Epoch 0/200] [Batch 556/938] [D loss: 0.277944] [G loss: 1.191607]\n",
      "[Epoch 0/200] [Batch 557/938] [D loss: 0.295869] [G loss: 1.296096]\n",
      "[Epoch 0/200] [Batch 558/938] [D loss: 0.287272] [G loss: 1.194717]\n",
      "[Epoch 0/200] [Batch 559/938] [D loss: 0.349888] [G loss: 1.416182]\n",
      "[Epoch 0/200] [Batch 560/938] [D loss: 0.389275] [G loss: 0.730096]\n",
      "[Epoch 0/200] [Batch 561/938] [D loss: 0.480599] [G loss: 2.299389]\n",
      "[Epoch 0/200] [Batch 562/938] [D loss: 0.495668] [G loss: 0.522562]\n",
      "[Epoch 0/200] [Batch 563/938] [D loss: 0.248808] [G loss: 1.707031]\n",
      "[Epoch 0/200] [Batch 564/938] [D loss: 0.221002] [G loss: 1.762136]\n",
      "[Epoch 0/200] [Batch 565/938] [D loss: 0.242955] [G loss: 1.171767]\n",
      "[Epoch 0/200] [Batch 566/938] [D loss: 0.228908] [G loss: 1.537831]\n",
      "[Epoch 0/200] [Batch 567/938] [D loss: 0.241603] [G loss: 1.453913]\n",
      "[Epoch 0/200] [Batch 568/938] [D loss: 0.260363] [G loss: 1.317868]\n",
      "[Epoch 0/200] [Batch 569/938] [D loss: 0.300343] [G loss: 1.321273]\n",
      "[Epoch 0/200] [Batch 570/938] [D loss: 0.327830] [G loss: 1.140429]\n",
      "[Epoch 0/200] [Batch 571/938] [D loss: 0.319146] [G loss: 1.209298]\n",
      "[Epoch 0/200] [Batch 572/938] [D loss: 0.314755] [G loss: 1.357761]\n",
      "[Epoch 0/200] [Batch 573/938] [D loss: 0.360979] [G loss: 0.942473]\n",
      "[Epoch 0/200] [Batch 574/938] [D loss: 0.392567] [G loss: 1.799942]\n",
      "[Epoch 0/200] [Batch 575/938] [D loss: 0.588917] [G loss: 0.430717]\n",
      "[Epoch 0/200] [Batch 576/938] [D loss: 0.648389] [G loss: 2.854002]\n",
      "[Epoch 0/200] [Batch 577/938] [D loss: 0.747864] [G loss: 0.269085]\n",
      "[Epoch 0/200] [Batch 578/938] [D loss: 0.274801] [G loss: 2.095021]\n",
      "[Epoch 0/200] [Batch 579/938] [D loss: 0.227310] [G loss: 1.922256]\n",
      "[Epoch 0/200] [Batch 580/938] [D loss: 0.290659] [G loss: 1.052244]\n",
      "[Epoch 0/200] [Batch 581/938] [D loss: 0.267540] [G loss: 1.609880]\n",
      "[Epoch 0/200] [Batch 582/938] [D loss: 0.292379] [G loss: 1.243676]\n",
      "[Epoch 0/200] [Batch 583/938] [D loss: 0.294134] [G loss: 1.404221]\n",
      "[Epoch 0/200] [Batch 584/938] [D loss: 0.358667] [G loss: 1.136523]\n",
      "[Epoch 0/200] [Batch 585/938] [D loss: 0.320116] [G loss: 1.201870]\n",
      "[Epoch 0/200] [Batch 586/938] [D loss: 0.317113] [G loss: 1.476516]\n",
      "[Epoch 0/200] [Batch 587/938] [D loss: 0.323674] [G loss: 0.999578]\n",
      "[Epoch 0/200] [Batch 588/938] [D loss: 0.350540] [G loss: 2.237553]\n",
      "[Epoch 0/200] [Batch 589/938] [D loss: 0.523420] [G loss: 0.496032]\n",
      "[Epoch 0/200] [Batch 590/938] [D loss: 0.495505] [G loss: 3.128644]\n",
      "[Epoch 0/200] [Batch 591/938] [D loss: 0.430352] [G loss: 0.660429]\n",
      "[Epoch 0/200] [Batch 592/938] [D loss: 0.203167] [G loss: 1.719066]\n",
      "[Epoch 0/200] [Batch 593/938] [D loss: 0.289692] [G loss: 2.439143]\n",
      "[Epoch 0/200] [Batch 594/938] [D loss: 0.364857] [G loss: 0.757945]\n",
      "[Epoch 0/200] [Batch 595/938] [D loss: 0.286950] [G loss: 2.459916]\n",
      "[Epoch 0/200] [Batch 596/938] [D loss: 0.311459] [G loss: 1.033884]\n",
      "[Epoch 0/200] [Batch 597/938] [D loss: 0.285787] [G loss: 1.709787]\n",
      "[Epoch 0/200] [Batch 598/938] [D loss: 0.306505] [G loss: 1.185949]\n",
      "[Epoch 0/200] [Batch 599/938] [D loss: 0.249122] [G loss: 1.642587]\n",
      "[Epoch 0/200] [Batch 600/938] [D loss: 0.277580] [G loss: 1.234808]\n",
      "[Epoch 0/200] [Batch 601/938] [D loss: 0.359440] [G loss: 1.594772]\n",
      "[Epoch 0/200] [Batch 602/938] [D loss: 0.493133] [G loss: 0.579414]\n",
      "[Epoch 0/200] [Batch 603/938] [D loss: 0.722620] [G loss: 3.417982]\n",
      "[Epoch 0/200] [Batch 604/938] [D loss: 0.876658] [G loss: 0.260734]\n",
      "[Epoch 0/200] [Batch 605/938] [D loss: 0.245490] [G loss: 2.377465]\n",
      "[Epoch 0/200] [Batch 606/938] [D loss: 0.267089] [G loss: 2.346744]\n",
      "[Epoch 0/200] [Batch 607/938] [D loss: 0.355835] [G loss: 0.834573]\n",
      "[Epoch 0/200] [Batch 608/938] [D loss: 0.223650] [G loss: 2.157724]\n",
      "[Epoch 0/200] [Batch 609/938] [D loss: 0.247335] [G loss: 1.441896]\n",
      "[Epoch 0/200] [Batch 610/938] [D loss: 0.305301] [G loss: 1.034571]\n",
      "[Epoch 0/200] [Batch 611/938] [D loss: 0.374915] [G loss: 2.028615]\n",
      "[Epoch 0/200] [Batch 612/938] [D loss: 0.563620] [G loss: 0.460722]\n",
      "[Epoch 0/200] [Batch 613/938] [D loss: 0.622448] [G loss: 2.700176]\n",
      "[Epoch 0/200] [Batch 614/938] [D loss: 0.723270] [G loss: 0.324732]\n",
      "[Epoch 0/200] [Batch 615/938] [D loss: 0.252571] [G loss: 2.160017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 616/938] [D loss: 0.227105] [G loss: 1.909815]\n",
      "[Epoch 0/200] [Batch 617/938] [D loss: 0.304615] [G loss: 1.003639]\n",
      "[Epoch 0/200] [Batch 618/938] [D loss: 0.267106] [G loss: 1.746211]\n",
      "[Epoch 0/200] [Batch 619/938] [D loss: 0.302701] [G loss: 1.120270]\n",
      "[Epoch 0/200] [Batch 620/938] [D loss: 0.315715] [G loss: 1.499144]\n",
      "[Epoch 0/200] [Batch 621/938] [D loss: 0.373961] [G loss: 0.896575]\n",
      "[Epoch 0/200] [Batch 622/938] [D loss: 0.435250] [G loss: 1.742104]\n",
      "[Epoch 0/200] [Batch 623/938] [D loss: 0.669013] [G loss: 0.369772]\n",
      "[Epoch 0/200] [Batch 624/938] [D loss: 0.671849] [G loss: 2.882887]\n",
      "[Epoch 0/200] [Batch 625/938] [D loss: 0.614325] [G loss: 0.449616]\n",
      "[Epoch 0/200] [Batch 626/938] [D loss: 0.243455] [G loss: 1.562909]\n",
      "[Epoch 0/200] [Batch 627/938] [D loss: 0.310735] [G loss: 2.197899]\n",
      "[Epoch 0/200] [Batch 628/938] [D loss: 0.381127] [G loss: 0.811134]\n",
      "[Epoch 0/200] [Batch 629/938] [D loss: 0.256430] [G loss: 1.594639]\n",
      "[Epoch 0/200] [Batch 630/938] [D loss: 0.332415] [G loss: 1.603201]\n",
      "[Epoch 0/200] [Batch 631/938] [D loss: 0.414987] [G loss: 0.702848]\n",
      "[Epoch 0/200] [Batch 632/938] [D loss: 0.398922] [G loss: 2.060346]\n",
      "[Epoch 0/200] [Batch 633/938] [D loss: 0.415613] [G loss: 0.727128]\n",
      "[Epoch 0/200] [Batch 634/938] [D loss: 0.272959] [G loss: 1.557935]\n",
      "[Epoch 0/200] [Batch 635/938] [D loss: 0.288206] [G loss: 1.436191]\n",
      "[Epoch 0/200] [Batch 636/938] [D loss: 0.317613] [G loss: 0.998141]\n",
      "[Epoch 0/200] [Batch 637/938] [D loss: 0.329265] [G loss: 1.694571]\n",
      "[Epoch 0/200] [Batch 638/938] [D loss: 0.398715] [G loss: 0.800828]\n",
      "[Epoch 0/200] [Batch 639/938] [D loss: 0.338643] [G loss: 1.699091]\n",
      "[Epoch 0/200] [Batch 640/938] [D loss: 0.376284] [G loss: 0.909349]\n",
      "[Epoch 0/200] [Batch 641/938] [D loss: 0.323792] [G loss: 1.468048]\n",
      "[Epoch 0/200] [Batch 642/938] [D loss: 0.337135] [G loss: 1.024961]\n",
      "[Epoch 0/200] [Batch 643/938] [D loss: 0.318694] [G loss: 1.364753]\n",
      "[Epoch 0/200] [Batch 644/938] [D loss: 0.309730] [G loss: 1.179969]\n",
      "[Epoch 0/200] [Batch 645/938] [D loss: 0.303026] [G loss: 1.357993]\n",
      "[Epoch 0/200] [Batch 646/938] [D loss: 0.306682] [G loss: 1.126546]\n",
      "[Epoch 0/200] [Batch 647/938] [D loss: 0.296960] [G loss: 1.508911]\n",
      "[Epoch 0/200] [Batch 648/938] [D loss: 0.288922] [G loss: 1.114731]\n",
      "[Epoch 0/200] [Batch 649/938] [D loss: 0.283762] [G loss: 1.599746]\n",
      "[Epoch 0/200] [Batch 650/938] [D loss: 0.297506] [G loss: 1.040659]\n",
      "[Epoch 0/200] [Batch 651/938] [D loss: 0.282855] [G loss: 1.848378]\n",
      "[Epoch 0/200] [Batch 652/938] [D loss: 0.302141] [G loss: 0.963941]\n",
      "[Epoch 0/200] [Batch 653/938] [D loss: 0.266867] [G loss: 2.048707]\n",
      "[Epoch 0/200] [Batch 654/938] [D loss: 0.321037] [G loss: 0.989339]\n",
      "[Epoch 0/200] [Batch 655/938] [D loss: 0.311289] [G loss: 1.669015]\n",
      "[Epoch 0/200] [Batch 656/938] [D loss: 0.339012] [G loss: 0.941972]\n",
      "[Epoch 0/200] [Batch 657/938] [D loss: 0.293317] [G loss: 1.819947]\n",
      "[Epoch 0/200] [Batch 658/938] [D loss: 0.307207] [G loss: 1.005762]\n",
      "[Epoch 0/200] [Batch 659/938] [D loss: 0.313440] [G loss: 2.118021]\n",
      "[Epoch 0/200] [Batch 660/938] [D loss: 0.409692] [G loss: 0.732556]\n",
      "[Epoch 0/200] [Batch 661/938] [D loss: 0.404213] [G loss: 2.420900]\n",
      "[Epoch 0/200] [Batch 662/938] [D loss: 0.503929] [G loss: 0.568455]\n",
      "[Epoch 0/200] [Batch 663/938] [D loss: 0.281506] [G loss: 2.418482]\n",
      "[Epoch 0/200] [Batch 664/938] [D loss: 0.258854] [G loss: 1.216792]\n",
      "[Epoch 0/200] [Batch 665/938] [D loss: 0.222554] [G loss: 1.527313]\n",
      "[Epoch 0/200] [Batch 666/938] [D loss: 0.258040] [G loss: 1.736676]\n",
      "[Epoch 0/200] [Batch 667/938] [D loss: 0.318442] [G loss: 0.950587]\n",
      "[Epoch 0/200] [Batch 668/938] [D loss: 0.341039] [G loss: 2.235155]\n",
      "[Epoch 0/200] [Batch 669/938] [D loss: 0.485705] [G loss: 0.568742]\n",
      "[Epoch 0/200] [Batch 670/938] [D loss: 0.441956] [G loss: 2.634792]\n",
      "[Epoch 0/200] [Batch 671/938] [D loss: 0.479186] [G loss: 0.589096]\n",
      "[Epoch 0/200] [Batch 672/938] [D loss: 0.289887] [G loss: 2.190491]\n",
      "[Epoch 0/200] [Batch 673/938] [D loss: 0.251040] [G loss: 1.244278]\n",
      "[Epoch 0/200] [Batch 674/938] [D loss: 0.216310] [G loss: 1.537464]\n",
      "[Epoch 0/200] [Batch 675/938] [D loss: 0.245171] [G loss: 1.758772]\n",
      "[Epoch 0/200] [Batch 676/938] [D loss: 0.274686] [G loss: 1.197008]\n",
      "[Epoch 0/200] [Batch 677/938] [D loss: 0.301643] [G loss: 1.661101]\n",
      "[Epoch 0/200] [Batch 678/938] [D loss: 0.366742] [G loss: 0.843467]\n",
      "[Epoch 0/200] [Batch 679/938] [D loss: 0.454394] [G loss: 2.441088]\n",
      "[Epoch 0/200] [Batch 680/938] [D loss: 0.732566] [G loss: 0.313212]\n",
      "[Epoch 0/200] [Batch 681/938] [D loss: 0.467434] [G loss: 2.818322]\n",
      "[Epoch 0/200] [Batch 682/938] [D loss: 0.344211] [G loss: 0.944869]\n",
      "[Epoch 0/200] [Batch 683/938] [D loss: 0.207617] [G loss: 1.550917]\n",
      "[Epoch 0/200] [Batch 684/938] [D loss: 0.269059] [G loss: 2.165154]\n",
      "[Epoch 0/200] [Batch 685/938] [D loss: 0.323501] [G loss: 0.925979]\n",
      "[Epoch 0/200] [Batch 686/938] [D loss: 0.237864] [G loss: 1.820498]\n",
      "[Epoch 0/200] [Batch 687/938] [D loss: 0.315835] [G loss: 1.561580]\n",
      "[Epoch 0/200] [Batch 688/938] [D loss: 0.424664] [G loss: 0.658561]\n",
      "[Epoch 0/200] [Batch 689/938] [D loss: 0.503065] [G loss: 2.760152]\n",
      "[Epoch 0/200] [Batch 690/938] [D loss: 0.587233] [G loss: 0.450516]\n",
      "[Epoch 0/200] [Batch 691/938] [D loss: 0.316489] [G loss: 2.159817]\n",
      "[Epoch 0/200] [Batch 692/938] [D loss: 0.307962] [G loss: 1.380697]\n",
      "[Epoch 0/200] [Batch 693/938] [D loss: 0.316793] [G loss: 1.091040]\n",
      "[Epoch 0/200] [Batch 694/938] [D loss: 0.350536] [G loss: 1.919176]\n",
      "[Epoch 0/200] [Batch 695/938] [D loss: 0.407859] [G loss: 0.759081]\n",
      "[Epoch 0/200] [Batch 696/938] [D loss: 0.400658] [G loss: 2.392251]\n",
      "[Epoch 0/200] [Batch 697/938] [D loss: 0.548710] [G loss: 0.520912]\n",
      "[Epoch 0/200] [Batch 698/938] [D loss: 0.506886] [G loss: 2.587672]\n",
      "[Epoch 0/200] [Batch 699/938] [D loss: 0.527547] [G loss: 0.516128]\n",
      "[Epoch 0/200] [Batch 700/938] [D loss: 0.354539] [G loss: 2.482558]\n",
      "[Epoch 0/200] [Batch 701/938] [D loss: 0.297192] [G loss: 1.062740]\n",
      "[Epoch 0/200] [Batch 702/938] [D loss: 0.232567] [G loss: 1.742283]\n",
      "[Epoch 0/200] [Batch 703/938] [D loss: 0.283446] [G loss: 1.610222]\n",
      "[Epoch 0/200] [Batch 704/938] [D loss: 0.332187] [G loss: 1.079406]\n",
      "[Epoch 0/200] [Batch 705/938] [D loss: 0.407351] [G loss: 1.890351]\n",
      "[Epoch 0/200] [Batch 706/938] [D loss: 0.621422] [G loss: 0.444652]\n",
      "[Epoch 0/200] [Batch 707/938] [D loss: 0.711206] [G loss: 2.984689]\n",
      "[Epoch 0/200] [Batch 708/938] [D loss: 0.717942] [G loss: 0.328061]\n",
      "[Epoch 0/200] [Batch 709/938] [D loss: 0.452298] [G loss: 2.212154]\n",
      "[Epoch 0/200] [Batch 710/938] [D loss: 0.321972] [G loss: 0.967712]\n",
      "[Epoch 0/200] [Batch 711/938] [D loss: 0.303579] [G loss: 1.677265]\n",
      "[Epoch 0/200] [Batch 712/938] [D loss: 0.313748] [G loss: 1.223437]\n",
      "[Epoch 0/200] [Batch 713/938] [D loss: 0.316410] [G loss: 1.347786]\n",
      "[Epoch 0/200] [Batch 714/938] [D loss: 0.301425] [G loss: 1.243223]\n",
      "[Epoch 0/200] [Batch 715/938] [D loss: 0.386809] [G loss: 1.512275]\n",
      "[Epoch 0/200] [Batch 716/938] [D loss: 0.463295] [G loss: 0.680041]\n",
      "[Epoch 0/200] [Batch 717/938] [D loss: 0.548780] [G loss: 2.341247]\n",
      "[Epoch 0/200] [Batch 718/938] [D loss: 0.774477] [G loss: 0.315407]\n",
      "[Epoch 0/200] [Batch 719/938] [D loss: 0.394863] [G loss: 2.373748]\n",
      "[Epoch 0/200] [Batch 720/938] [D loss: 0.268727] [G loss: 1.400594]\n",
      "[Epoch 0/200] [Batch 721/938] [D loss: 0.267991] [G loss: 1.229394]\n",
      "[Epoch 0/200] [Batch 722/938] [D loss: 0.310937] [G loss: 1.924011]\n",
      "[Epoch 0/200] [Batch 723/938] [D loss: 0.329278] [G loss: 1.016474]\n",
      "[Epoch 0/200] [Batch 724/938] [D loss: 0.300615] [G loss: 1.622531]\n",
      "[Epoch 0/200] [Batch 725/938] [D loss: 0.355947] [G loss: 1.053964]\n",
      "[Epoch 0/200] [Batch 726/938] [D loss: 0.294378] [G loss: 1.394351]\n",
      "[Epoch 0/200] [Batch 727/938] [D loss: 0.335689] [G loss: 1.378947]\n",
      "[Epoch 0/200] [Batch 728/938] [D loss: 0.399961] [G loss: 0.946779]\n",
      "[Epoch 0/200] [Batch 729/938] [D loss: 0.349449] [G loss: 1.756945]\n",
      "[Epoch 0/200] [Batch 730/938] [D loss: 0.403714] [G loss: 0.756100]\n",
      "[Epoch 0/200] [Batch 731/938] [D loss: 0.438272] [G loss: 2.666207]\n",
      "[Epoch 0/200] [Batch 732/938] [D loss: 0.452846] [G loss: 0.633094]\n",
      "[Epoch 0/200] [Batch 733/938] [D loss: 0.309806] [G loss: 2.500947]\n",
      "[Epoch 0/200] [Batch 734/938] [D loss: 0.272448] [G loss: 1.254876]\n",
      "[Epoch 0/200] [Batch 735/938] [D loss: 0.226163] [G loss: 1.681656]\n",
      "[Epoch 0/200] [Batch 736/938] [D loss: 0.231819] [G loss: 1.657804]\n",
      "[Epoch 0/200] [Batch 737/938] [D loss: 0.265145] [G loss: 1.422599]\n",
      "[Epoch 0/200] [Batch 738/938] [D loss: 0.299733] [G loss: 1.311967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 739/938] [D loss: 0.334191] [G loss: 1.302625]\n",
      "[Epoch 0/200] [Batch 740/938] [D loss: 0.400838] [G loss: 1.209828]\n",
      "[Epoch 0/200] [Batch 741/938] [D loss: 0.353809] [G loss: 1.070068]\n",
      "[Epoch 0/200] [Batch 742/938] [D loss: 0.307939] [G loss: 1.591563]\n",
      "[Epoch 0/200] [Batch 743/938] [D loss: 0.338101] [G loss: 0.979475]\n",
      "[Epoch 0/200] [Batch 744/938] [D loss: 0.308457] [G loss: 1.926819]\n",
      "[Epoch 0/200] [Batch 745/938] [D loss: 0.416013] [G loss: 0.716929]\n",
      "[Epoch 0/200] [Batch 746/938] [D loss: 0.463838] [G loss: 2.741468]\n",
      "[Epoch 0/200] [Batch 747/938] [D loss: 0.698749] [G loss: 0.332816]\n",
      "[Epoch 0/200] [Batch 748/938] [D loss: 0.552742] [G loss: 3.298922]\n",
      "[Epoch 0/200] [Batch 749/938] [D loss: 0.383815] [G loss: 0.766526]\n",
      "[Epoch 0/200] [Batch 750/938] [D loss: 0.192032] [G loss: 1.820749]\n",
      "[Epoch 0/200] [Batch 751/938] [D loss: 0.200122] [G loss: 2.133663]\n",
      "[Epoch 0/200] [Batch 752/938] [D loss: 0.237918] [G loss: 1.344108]\n",
      "[Epoch 0/200] [Batch 753/938] [D loss: 0.280531] [G loss: 1.551406]\n",
      "[Epoch 0/200] [Batch 754/938] [D loss: 0.342170] [G loss: 1.140670]\n",
      "[Epoch 0/200] [Batch 755/938] [D loss: 0.327408] [G loss: 1.377436]\n",
      "[Epoch 0/200] [Batch 756/938] [D loss: 0.368506] [G loss: 1.114776]\n",
      "[Epoch 0/200] [Batch 757/938] [D loss: 0.352294] [G loss: 1.299588]\n",
      "[Epoch 0/200] [Batch 758/938] [D loss: 0.315669] [G loss: 1.043018]\n",
      "[Epoch 0/200] [Batch 759/938] [D loss: 0.307075] [G loss: 2.072663]\n",
      "[Epoch 0/200] [Batch 760/938] [D loss: 0.411293] [G loss: 0.672388]\n",
      "[Epoch 0/200] [Batch 761/938] [D loss: 0.439825] [G loss: 2.681731]\n",
      "[Epoch 0/200] [Batch 762/938] [D loss: 0.573533] [G loss: 0.444044]\n",
      "[Epoch 0/200] [Batch 763/938] [D loss: 0.399964] [G loss: 2.990876]\n",
      "[Epoch 0/200] [Batch 764/938] [D loss: 0.267152] [G loss: 1.082039]\n",
      "[Epoch 0/200] [Batch 765/938] [D loss: 0.235098] [G loss: 1.452478]\n",
      "[Epoch 0/200] [Batch 766/938] [D loss: 0.257250] [G loss: 1.852818]\n",
      "[Epoch 0/200] [Batch 767/938] [D loss: 0.282672] [G loss: 1.169228]\n",
      "[Epoch 0/200] [Batch 768/938] [D loss: 0.319663] [G loss: 1.671288]\n",
      "[Epoch 0/200] [Batch 769/938] [D loss: 0.414949] [G loss: 0.911749]\n",
      "[Epoch 0/200] [Batch 770/938] [D loss: 0.347863] [G loss: 1.730259]\n",
      "[Epoch 0/200] [Batch 771/938] [D loss: 0.391302] [G loss: 0.798093]\n",
      "[Epoch 0/200] [Batch 772/938] [D loss: 0.403195] [G loss: 2.185251]\n",
      "[Epoch 0/200] [Batch 773/938] [D loss: 0.473364] [G loss: 0.593898]\n",
      "[Epoch 0/200] [Batch 774/938] [D loss: 0.414065] [G loss: 2.592334]\n",
      "[Epoch 0/200] [Batch 775/938] [D loss: 0.376986] [G loss: 0.731596]\n",
      "[Epoch 0/200] [Batch 776/938] [D loss: 0.262958] [G loss: 2.144784]\n",
      "[Epoch 0/200] [Batch 777/938] [D loss: 0.228982] [G loss: 1.291699]\n",
      "[Epoch 0/200] [Batch 778/938] [D loss: 0.236788] [G loss: 1.470299]\n",
      "[Epoch 0/200] [Batch 779/938] [D loss: 0.276106] [G loss: 1.538069]\n",
      "[Epoch 0/200] [Batch 780/938] [D loss: 0.314563] [G loss: 1.022174]\n",
      "[Epoch 0/200] [Batch 781/938] [D loss: 0.327729] [G loss: 2.028250]\n",
      "[Epoch 0/200] [Batch 782/938] [D loss: 0.472095] [G loss: 0.624957]\n",
      "[Epoch 0/200] [Batch 783/938] [D loss: 0.696378] [G loss: 2.878658]\n",
      "[Epoch 0/200] [Batch 784/938] [D loss: 0.985919] [G loss: 0.225932]\n",
      "[Epoch 0/200] [Batch 785/938] [D loss: 0.312715] [G loss: 2.131912]\n",
      "[Epoch 0/200] [Batch 786/938] [D loss: 0.239510] [G loss: 1.686237]\n",
      "[Epoch 0/200] [Batch 787/938] [D loss: 0.311987] [G loss: 0.995522]\n",
      "[Epoch 0/200] [Batch 788/938] [D loss: 0.274197] [G loss: 1.779185]\n",
      "[Epoch 0/200] [Batch 789/938] [D loss: 0.288799] [G loss: 1.050590]\n",
      "[Epoch 0/200] [Batch 790/938] [D loss: 0.242000] [G loss: 1.723179]\n",
      "[Epoch 0/200] [Batch 791/938] [D loss: 0.302978] [G loss: 1.219501]\n",
      "[Epoch 0/200] [Batch 792/938] [D loss: 0.366666] [G loss: 1.125168]\n",
      "[Epoch 0/200] [Batch 793/938] [D loss: 0.320400] [G loss: 1.202801]\n",
      "[Epoch 0/200] [Batch 794/938] [D loss: 0.319469] [G loss: 1.295793]\n",
      "[Epoch 0/200] [Batch 795/938] [D loss: 0.305476] [G loss: 1.095285]\n",
      "[Epoch 0/200] [Batch 796/938] [D loss: 0.300271] [G loss: 1.757205]\n",
      "[Epoch 0/200] [Batch 797/938] [D loss: 0.400467] [G loss: 0.697672]\n",
      "[Epoch 0/200] [Batch 798/938] [D loss: 0.529746] [G loss: 2.843448]\n",
      "[Epoch 0/200] [Batch 799/938] [D loss: 0.696894] [G loss: 0.326637]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.346264] [G loss: 2.879220]\n",
      "[Epoch 0/200] [Batch 801/938] [D loss: 0.207548] [G loss: 1.394942]\n",
      "[Epoch 0/200] [Batch 802/938] [D loss: 0.224113] [G loss: 1.252699]\n",
      "[Epoch 0/200] [Batch 803/938] [D loss: 0.186675] [G loss: 1.995002]\n",
      "[Epoch 0/200] [Batch 804/938] [D loss: 0.233465] [G loss: 1.542678]\n",
      "[Epoch 0/200] [Batch 805/938] [D loss: 0.240366] [G loss: 1.259771]\n",
      "[Epoch 0/200] [Batch 806/938] [D loss: 0.315909] [G loss: 1.791886]\n",
      "[Epoch 0/200] [Batch 807/938] [D loss: 0.400742] [G loss: 0.737926]\n",
      "[Epoch 0/200] [Batch 808/938] [D loss: 0.511640] [G loss: 2.777684]\n",
      "[Epoch 0/200] [Batch 809/938] [D loss: 0.713014] [G loss: 0.364590]\n",
      "[Epoch 0/200] [Batch 810/938] [D loss: 0.360873] [G loss: 2.858155]\n",
      "[Epoch 0/200] [Batch 811/938] [D loss: 0.268811] [G loss: 1.213380]\n",
      "[Epoch 0/200] [Batch 812/938] [D loss: 0.248535] [G loss: 1.206255]\n",
      "[Epoch 0/200] [Batch 813/938] [D loss: 0.209146] [G loss: 2.086739]\n",
      "[Epoch 0/200] [Batch 814/938] [D loss: 0.237718] [G loss: 1.333013]\n",
      "[Epoch 0/200] [Batch 815/938] [D loss: 0.251179] [G loss: 1.471857]\n",
      "[Epoch 0/200] [Batch 816/938] [D loss: 0.300557] [G loss: 1.379685]\n",
      "[Epoch 0/200] [Batch 817/938] [D loss: 0.336955] [G loss: 1.113904]\n",
      "[Epoch 0/200] [Batch 818/938] [D loss: 0.316001] [G loss: 1.570388]\n",
      "[Epoch 0/200] [Batch 819/938] [D loss: 0.342030] [G loss: 0.879287]\n",
      "[Epoch 0/200] [Batch 820/938] [D loss: 0.473049] [G loss: 2.612196]\n",
      "[Epoch 0/200] [Batch 821/938] [D loss: 0.754140] [G loss: 0.303131]\n",
      "[Epoch 0/200] [Batch 822/938] [D loss: 0.510460] [G loss: 3.419027]\n",
      "[Epoch 0/200] [Batch 823/938] [D loss: 0.327929] [G loss: 0.953334]\n",
      "[Epoch 0/200] [Batch 824/938] [D loss: 0.166421] [G loss: 1.681634]\n",
      "[Epoch 0/200] [Batch 825/938] [D loss: 0.198245] [G loss: 2.349949]\n",
      "[Epoch 0/200] [Batch 826/938] [D loss: 0.270854] [G loss: 1.253261]\n",
      "[Epoch 0/200] [Batch 827/938] [D loss: 0.238906] [G loss: 1.327051]\n",
      "[Epoch 0/200] [Batch 828/938] [D loss: 0.265627] [G loss: 1.914208]\n",
      "[Epoch 0/200] [Batch 829/938] [D loss: 0.352064] [G loss: 0.931657]\n",
      "[Epoch 0/200] [Batch 830/938] [D loss: 0.376518] [G loss: 2.043102]\n",
      "[Epoch 0/200] [Batch 831/938] [D loss: 0.588238] [G loss: 0.408545]\n",
      "[Epoch 0/200] [Batch 832/938] [D loss: 0.677725] [G loss: 3.793260]\n",
      "[Epoch 0/200] [Batch 833/938] [D loss: 0.660265] [G loss: 0.346788]\n",
      "[Epoch 0/200] [Batch 834/938] [D loss: 0.211913] [G loss: 2.583039]\n",
      "[Epoch 0/200] [Batch 835/938] [D loss: 0.185145] [G loss: 2.191222]\n",
      "[Epoch 0/200] [Batch 836/938] [D loss: 0.262356] [G loss: 1.145018]\n",
      "[Epoch 0/200] [Batch 837/938] [D loss: 0.212248] [G loss: 1.859519]\n",
      "[Epoch 0/200] [Batch 838/938] [D loss: 0.238529] [G loss: 1.551811]\n",
      "[Epoch 0/200] [Batch 839/938] [D loss: 0.323857] [G loss: 1.258016]\n",
      "[Epoch 0/200] [Batch 840/938] [D loss: 0.325189] [G loss: 1.239512]\n",
      "[Epoch 0/200] [Batch 841/938] [D loss: 0.370088] [G loss: 1.262743]\n",
      "[Epoch 0/200] [Batch 842/938] [D loss: 0.405878] [G loss: 0.846553]\n",
      "[Epoch 0/200] [Batch 843/938] [D loss: 0.518954] [G loss: 2.091486]\n",
      "[Epoch 0/200] [Batch 844/938] [D loss: 1.104185] [G loss: 0.122732]\n",
      "[Epoch 0/200] [Batch 845/938] [D loss: 1.057077] [G loss: 4.179874]\n",
      "[Epoch 0/200] [Batch 846/938] [D loss: 0.602259] [G loss: 0.628731]\n",
      "[Epoch 0/200] [Batch 847/938] [D loss: 0.209356] [G loss: 1.476393]\n",
      "[Epoch 0/200] [Batch 848/938] [D loss: 0.231100] [G loss: 2.998632]\n",
      "[Epoch 0/200] [Batch 849/938] [D loss: 0.194721] [G loss: 1.731149]\n",
      "[Epoch 0/200] [Batch 850/938] [D loss: 0.306627] [G loss: 0.955684]\n",
      "[Epoch 0/200] [Batch 851/938] [D loss: 0.335121] [G loss: 2.115316]\n",
      "[Epoch 0/200] [Batch 852/938] [D loss: 0.384710] [G loss: 0.827465]\n",
      "[Epoch 0/200] [Batch 853/938] [D loss: 0.389935] [G loss: 1.838246]\n",
      "[Epoch 0/200] [Batch 854/938] [D loss: 0.413105] [G loss: 0.740909]\n",
      "[Epoch 0/200] [Batch 855/938] [D loss: 0.418995] [G loss: 2.227844]\n",
      "[Epoch 0/200] [Batch 856/938] [D loss: 0.470975] [G loss: 0.632085]\n",
      "[Epoch 0/200] [Batch 857/938] [D loss: 0.432134] [G loss: 2.508368]\n",
      "[Epoch 0/200] [Batch 858/938] [D loss: 0.409784] [G loss: 0.732615]\n",
      "[Epoch 0/200] [Batch 859/938] [D loss: 0.314608] [G loss: 2.227478]\n",
      "[Epoch 0/200] [Batch 860/938] [D loss: 0.297651] [G loss: 1.078229]\n",
      "[Epoch 0/200] [Batch 861/938] [D loss: 0.266826] [G loss: 1.775572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 862/938] [D loss: 0.307848] [G loss: 1.261026]\n",
      "[Epoch 0/200] [Batch 863/938] [D loss: 0.326571] [G loss: 1.268378]\n",
      "[Epoch 0/200] [Batch 864/938] [D loss: 0.330730] [G loss: 1.418857]\n",
      "[Epoch 0/200] [Batch 865/938] [D loss: 0.334312] [G loss: 1.003247]\n",
      "[Epoch 0/200] [Batch 866/938] [D loss: 0.528911] [G loss: 2.254957]\n",
      "[Epoch 0/200] [Batch 867/938] [D loss: 1.002672] [G loss: 0.174481]\n",
      "[Epoch 0/200] [Batch 868/938] [D loss: 0.673067] [G loss: 3.355190]\n",
      "[Epoch 0/200] [Batch 869/938] [D loss: 0.437421] [G loss: 0.812459]\n",
      "[Epoch 0/200] [Batch 870/938] [D loss: 0.217596] [G loss: 1.780616]\n",
      "[Epoch 0/200] [Batch 871/938] [D loss: 0.288327] [G loss: 1.959931]\n",
      "[Epoch 0/200] [Batch 872/938] [D loss: 0.392826] [G loss: 0.757249]\n",
      "[Epoch 0/200] [Batch 873/938] [D loss: 0.349561] [G loss: 2.135210]\n",
      "[Epoch 0/200] [Batch 874/938] [D loss: 0.386424] [G loss: 0.824310]\n",
      "[Epoch 0/200] [Batch 875/938] [D loss: 0.375300] [G loss: 2.071775]\n",
      "[Epoch 0/200] [Batch 876/938] [D loss: 0.431084] [G loss: 0.711156]\n",
      "[Epoch 0/200] [Batch 877/938] [D loss: 0.482014] [G loss: 2.481896]\n",
      "[Epoch 0/200] [Batch 878/938] [D loss: 0.542875] [G loss: 0.495810]\n",
      "[Epoch 0/200] [Batch 879/938] [D loss: 0.413704] [G loss: 2.545120]\n",
      "[Epoch 0/200] [Batch 880/938] [D loss: 0.340641] [G loss: 0.948381]\n",
      "[Epoch 0/200] [Batch 881/938] [D loss: 0.341509] [G loss: 1.708282]\n",
      "[Epoch 0/200] [Batch 882/938] [D loss: 0.386373] [G loss: 0.835311]\n",
      "[Epoch 0/200] [Batch 883/938] [D loss: 0.311307] [G loss: 2.148770]\n",
      "[Epoch 0/200] [Batch 884/938] [D loss: 0.331598] [G loss: 1.040155]\n",
      "[Epoch 0/200] [Batch 885/938] [D loss: 0.342001] [G loss: 1.724258]\n",
      "[Epoch 0/200] [Batch 886/938] [D loss: 0.381530] [G loss: 0.879939]\n",
      "[Epoch 0/200] [Batch 887/938] [D loss: 0.417548] [G loss: 2.142988]\n",
      "[Epoch 0/200] [Batch 888/938] [D loss: 0.517083] [G loss: 0.561953]\n",
      "[Epoch 0/200] [Batch 889/938] [D loss: 0.408460] [G loss: 2.551494]\n",
      "[Epoch 0/200] [Batch 890/938] [D loss: 0.384409] [G loss: 0.855154]\n",
      "[Epoch 0/200] [Batch 891/938] [D loss: 0.268074] [G loss: 1.961648]\n",
      "[Epoch 0/200] [Batch 892/938] [D loss: 0.281868] [G loss: 1.319080]\n",
      "[Epoch 0/200] [Batch 893/938] [D loss: 0.341043] [G loss: 1.348585]\n",
      "[Epoch 0/200] [Batch 894/938] [D loss: 0.295962] [G loss: 1.176225]\n",
      "[Epoch 0/200] [Batch 895/938] [D loss: 0.385976] [G loss: 1.934240]\n",
      "[Epoch 0/200] [Batch 896/938] [D loss: 0.497402] [G loss: 0.546898]\n",
      "[Epoch 0/200] [Batch 897/938] [D loss: 0.554032] [G loss: 3.327480]\n",
      "[Epoch 0/200] [Batch 898/938] [D loss: 0.414987] [G loss: 0.711839]\n",
      "[Epoch 0/200] [Batch 899/938] [D loss: 0.261491] [G loss: 2.184726]\n",
      "[Epoch 0/200] [Batch 900/938] [D loss: 0.226340] [G loss: 1.523433]\n",
      "[Epoch 0/200] [Batch 901/938] [D loss: 0.256675] [G loss: 1.381598]\n",
      "[Epoch 0/200] [Batch 902/938] [D loss: 0.264037] [G loss: 1.548991]\n",
      "[Epoch 0/200] [Batch 903/938] [D loss: 0.298015] [G loss: 1.323352]\n",
      "[Epoch 0/200] [Batch 904/938] [D loss: 0.303218] [G loss: 1.244670]\n",
      "[Epoch 0/200] [Batch 905/938] [D loss: 0.353026] [G loss: 1.739197]\n",
      "[Epoch 0/200] [Batch 906/938] [D loss: 0.504288] [G loss: 0.583775]\n",
      "[Epoch 0/200] [Batch 907/938] [D loss: 0.851153] [G loss: 3.220785]\n",
      "[Epoch 0/200] [Batch 908/938] [D loss: 1.215806] [G loss: 0.114586]\n",
      "[Epoch 0/200] [Batch 909/938] [D loss: 0.295303] [G loss: 2.544374]\n",
      "[Epoch 0/200] [Batch 910/938] [D loss: 0.343698] [G loss: 2.606556]\n",
      "[Epoch 0/200] [Batch 911/938] [D loss: 0.430167] [G loss: 0.862740]\n",
      "[Epoch 0/200] [Batch 912/938] [D loss: 0.261291] [G loss: 1.855729]\n",
      "[Epoch 0/200] [Batch 913/938] [D loss: 0.265400] [G loss: 1.566815]\n",
      "[Epoch 0/200] [Batch 914/938] [D loss: 0.309068] [G loss: 1.045851]\n",
      "[Epoch 0/200] [Batch 915/938] [D loss: 0.245154] [G loss: 1.829718]\n",
      "[Epoch 0/200] [Batch 916/938] [D loss: 0.284449] [G loss: 1.387336]\n",
      "[Epoch 0/200] [Batch 917/938] [D loss: 0.317887] [G loss: 1.181301]\n",
      "[Epoch 0/200] [Batch 918/938] [D loss: 0.369030] [G loss: 1.513832]\n",
      "[Epoch 0/200] [Batch 919/938] [D loss: 0.417088] [G loss: 0.730560]\n",
      "[Epoch 0/200] [Batch 920/938] [D loss: 0.436856] [G loss: 2.568455]\n",
      "[Epoch 0/200] [Batch 921/938] [D loss: 0.503498] [G loss: 0.524937]\n",
      "[Epoch 0/200] [Batch 922/938] [D loss: 0.431094] [G loss: 2.745852]\n",
      "[Epoch 0/200] [Batch 923/938] [D loss: 0.370433] [G loss: 0.748087]\n",
      "[Epoch 0/200] [Batch 924/938] [D loss: 0.250648] [G loss: 2.091583]\n",
      "[Epoch 0/200] [Batch 925/938] [D loss: 0.228284] [G loss: 1.419104]\n",
      "[Epoch 0/200] [Batch 926/938] [D loss: 0.251888] [G loss: 1.439721]\n",
      "[Epoch 0/200] [Batch 927/938] [D loss: 0.266421] [G loss: 1.526001]\n",
      "[Epoch 0/200] [Batch 928/938] [D loss: 0.254392] [G loss: 1.265996]\n",
      "[Epoch 0/200] [Batch 929/938] [D loss: 0.276959] [G loss: 1.916266]\n",
      "[Epoch 0/200] [Batch 930/938] [D loss: 0.344861] [G loss: 0.839777]\n",
      "[Epoch 0/200] [Batch 931/938] [D loss: 0.407421] [G loss: 2.461499]\n",
      "[Epoch 0/200] [Batch 932/938] [D loss: 0.516274] [G loss: 0.500078]\n",
      "[Epoch 0/200] [Batch 933/938] [D loss: 0.431834] [G loss: 2.990072]\n",
      "[Epoch 0/200] [Batch 934/938] [D loss: 0.279603] [G loss: 1.004466]\n",
      "[Epoch 0/200] [Batch 935/938] [D loss: 0.209817] [G loss: 1.638032]\n",
      "[Epoch 0/200] [Batch 936/938] [D loss: 0.233440] [G loss: 1.739537]\n",
      "[Epoch 0/200] [Batch 937/938] [D loss: 0.237959] [G loss: 1.231701]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.210714] [G loss: 1.950489]\n",
      "[Epoch 1/200] [Batch 1/938] [D loss: 0.252840] [G loss: 1.383000]\n",
      "[Epoch 1/200] [Batch 2/938] [D loss: 0.319009] [G loss: 1.313184]\n",
      "[Epoch 1/200] [Batch 3/938] [D loss: 0.312360] [G loss: 1.223543]\n",
      "[Epoch 1/200] [Batch 4/938] [D loss: 0.307020] [G loss: 1.480637]\n",
      "[Epoch 1/200] [Batch 5/938] [D loss: 0.334796] [G loss: 0.946188]\n",
      "[Epoch 1/200] [Batch 6/938] [D loss: 0.311734] [G loss: 2.223347]\n",
      "[Epoch 1/200] [Batch 7/938] [D loss: 0.395941] [G loss: 0.705969]\n",
      "[Epoch 1/200] [Batch 8/938] [D loss: 0.448663] [G loss: 2.843541]\n",
      "[Epoch 1/200] [Batch 9/938] [D loss: 0.397910] [G loss: 0.681623]\n",
      "[Epoch 1/200] [Batch 10/938] [D loss: 0.231828] [G loss: 2.382628]\n",
      "[Epoch 1/200] [Batch 11/938] [D loss: 0.164467] [G loss: 1.683018]\n",
      "[Epoch 1/200] [Batch 12/938] [D loss: 0.189552] [G loss: 1.615183]\n",
      "[Epoch 1/200] [Batch 13/938] [D loss: 0.191063] [G loss: 1.878135]\n",
      "[Epoch 1/200] [Batch 14/938] [D loss: 0.180393] [G loss: 1.615694]\n",
      "[Epoch 1/200] [Batch 15/938] [D loss: 0.198173] [G loss: 1.822708]\n",
      "[Epoch 1/200] [Batch 16/938] [D loss: 0.229964] [G loss: 1.430806]\n",
      "[Epoch 1/200] [Batch 17/938] [D loss: 0.245676] [G loss: 1.689179]\n",
      "[Epoch 1/200] [Batch 18/938] [D loss: 0.235444] [G loss: 1.232220]\n",
      "[Epoch 1/200] [Batch 19/938] [D loss: 0.331376] [G loss: 2.313518]\n",
      "[Epoch 1/200] [Batch 20/938] [D loss: 0.577953] [G loss: 0.407309]\n",
      "[Epoch 1/200] [Batch 21/938] [D loss: 0.647792] [G loss: 4.221428]\n",
      "[Epoch 1/200] [Batch 22/938] [D loss: 0.274688] [G loss: 0.983902]\n",
      "[Epoch 1/200] [Batch 23/938] [D loss: 0.157348] [G loss: 1.679785]\n",
      "[Epoch 1/200] [Batch 24/938] [D loss: 0.137212] [G loss: 2.474780]\n",
      "[Epoch 1/200] [Batch 25/938] [D loss: 0.171879] [G loss: 1.935166]\n",
      "[Epoch 1/200] [Batch 26/938] [D loss: 0.217753] [G loss: 1.308360]\n",
      "[Epoch 1/200] [Batch 27/938] [D loss: 0.185321] [G loss: 1.997832]\n",
      "[Epoch 1/200] [Batch 28/938] [D loss: 0.212786] [G loss: 1.606702]\n",
      "[Epoch 1/200] [Batch 29/938] [D loss: 0.231344] [G loss: 1.418796]\n",
      "[Epoch 1/200] [Batch 30/938] [D loss: 0.261580] [G loss: 1.752780]\n",
      "[Epoch 1/200] [Batch 31/938] [D loss: 0.279764] [G loss: 1.105036]\n",
      "[Epoch 1/200] [Batch 32/938] [D loss: 0.257684] [G loss: 2.165535]\n",
      "[Epoch 1/200] [Batch 33/938] [D loss: 0.312405] [G loss: 0.915086]\n",
      "[Epoch 1/200] [Batch 34/938] [D loss: 0.344886] [G loss: 2.710618]\n",
      "[Epoch 1/200] [Batch 35/938] [D loss: 0.442090] [G loss: 0.597575]\n",
      "[Epoch 1/200] [Batch 36/938] [D loss: 0.398268] [G loss: 3.311223]\n",
      "[Epoch 1/200] [Batch 37/938] [D loss: 0.299321] [G loss: 0.914000]\n",
      "[Epoch 1/200] [Batch 38/938] [D loss: 0.146122] [G loss: 2.247489]\n",
      "[Epoch 1/200] [Batch 39/938] [D loss: 0.136962] [G loss: 2.147125]\n",
      "[Epoch 1/200] [Batch 40/938] [D loss: 0.191845] [G loss: 1.652640]\n",
      "[Epoch 1/200] [Batch 41/938] [D loss: 0.206124] [G loss: 1.517841]\n",
      "[Epoch 1/200] [Batch 42/938] [D loss: 0.205337] [G loss: 1.719178]\n",
      "[Epoch 1/200] [Batch 43/938] [D loss: 0.238860] [G loss: 1.657713]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ac7a7528f924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Generate a batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Loss measures generator's ability to fool the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9bf869ad6462>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, opt.n_epochs, i, len(dataloader),\n",
    "                                                            d_loss.item(), g_loss.item()))\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
